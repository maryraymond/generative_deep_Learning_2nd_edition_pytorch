{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\"\n",
    "exp_dir = working_dir + \"/notebooks/08_diffusion/01_ddm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)\n",
    "\n",
    "utils_path = os.path.abspath(exp_dir)\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "DATASET_REPETITIONS = 5\n",
    "LOAD_MODEL = False\n",
    "\n",
    "NOISE_EMBEDDING_SIZE = 32\n",
    "PLOT_DIFFUSION_STEPS = 20\n",
    "\n",
    "# optimization\n",
    "EMA = 0.999\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/pytorch-challange-flower-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom dataset to repeat the data\n",
    "class RepeatedDataset(Dataset):\n",
    "    def __init__(self, dataset, num_repeats):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.num_repeats\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        orginal_idx = index % len(self.dataset)\n",
    "        return self.dataset[orginal_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "\n",
    "repeated_dataset = RepeatedDataset(dataset=dataset, num_repeats=DATASET_REPETITIONS)\n",
    "\n",
    "train_dataset = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Size of orginal dataset = {len(dataset)}\")\n",
    "print(f\"Size of repeated dataset = {len(repeated_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataset)\n",
    "sample_images, _ = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(sample_images))\n",
    "print(torch.max(sample_images))\n",
    "images_mean = torch.mean(sample_images)\n",
    "print(images_mean)\n",
    "images_var = torch.var(sample_images)\n",
    "print(images_var)\n",
    "images_std = torch.std(sample_images)\n",
    "print(images_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that when we normalize the images we get the input images x0 with mean = 0 and var = 1\n",
    "normalized_images = (sample_images - images_mean) / images_std\n",
    "print(\"normalized images mean = \", torch.mean(normalized_images))\n",
    "print(\"normalized images var = \", torch.var(normalized_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Diffusion schedules <a name=\"diffusion_schedules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_diffusion_schduler(diffusion_times):\n",
    "    min_rate = 0.0001\n",
    "    max_rate = 0.02\n",
    "    beta = min_rate + (diffusion_times * (max_rate - min_rate))\n",
    "    alfa = 1 - beta\n",
    "    alfa_bar = torch.cumprod(alfa, dim=0)\n",
    "    signal_rate = torch.sqrt(alfa_bar)\n",
    "    noise_rate = torch.sqrt((1 - alfa_bar))\n",
    "\n",
    "    return noise_rate, signal_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schduler(diffusion_times):\n",
    "    signal_rate = torch.cos(diffusion_times * (torch.pi / 2))\n",
    "    noise_rate = torch.sin(diffusion_times * (torch.pi / 2))\n",
    "\n",
    "    return noise_rate, signal_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_cosine_schduler(diffusion_times):\n",
    "    min_signal_rate = torch.as_tensor(0.02)\n",
    "    max_signal_rate = torch.as_tensor(0.95)\n",
    "    start_angle = torch.acos(max_signal_rate)\n",
    "    end_angle = torch.acos(min_signal_rate)\n",
    "\n",
    "    theta = start_angle + (diffusion_times * (end_angle - start_angle))\n",
    "\n",
    "    signal_rate = torch.cos(theta)\n",
    "    noise_rate = torch.sin(theta)\n",
    "\n",
    "    return noise_rate, signal_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "\n",
    "diffusion_times = torch.as_tensor([t/T for t in range(T)])\n",
    "print(diffusion_times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_noise_rates, linear_signal_rates  = linear_diffusion_schduler(diffusion_times)\n",
    "\n",
    "cosine_noise_rates, cosine_signal_rates = cosine_schduler(diffusion_times)\n",
    "\n",
    "offset_cosine_noise_rates, offset_cosine_signal_rates = offset_cosine_schduler(diffusion_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diffusion_times, linear_signal_rates**2, linewidth=1.5, label=\"linear\")\n",
    "plt.plot( diffusion_times, cosine_signal_rates**2, linewidth=1.5, label=\"cosine\")\n",
    "plt.plot(diffusion_times, offset_cosine_signal_rates**2, linewidth=1.5, label=\"offset_cosine\")\n",
    "\n",
    "plt.xlabel(\"t/T\")\n",
    "plt.ylabel(r\"$\\bar{\\alpha_t}$ (signal)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diffusion_times, linear_noise_rates**2, linewidth=1.2, label=\"linear\")\n",
    "plt.plot(diffusion_times, cosine_noise_rates**2, linewidth=1.2, label=\"cosine\")\n",
    "plt.plot(diffusion_times, offset_cosine_noise_rates**2, linewidth=1.2, label=\"offset cosine\")\n",
    "\n",
    "plt.xlabel(\"t/T\")\n",
    "plt.ylabel(r\"1 - $\\bar{\\alpha_t}$ (noise)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(Module):\n",
    "    def __init__(self, l):\n",
    "        super().__init__()\n",
    "        self.l = l\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedding = []\n",
    "        f = torch.log(torch.as_tensor(1000)) / (self.l - 1)\n",
    "        mult = torch.arange(0, self.l, 1)\n",
    "        freqs = mult * f\n",
    "        \n",
    "        embedding.extend(torch.sin(2* torch.pi * torch.exp(freqs) * x))\n",
    "        embedding.extend(torch.cos(2* torch.pi * torch.exp(freqs) * x))\n",
    "        \n",
    "        return torch.as_tensor(torch.stack(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x, l, device=torch.device(\"cpu\")):\n",
    "    embedding = []\n",
    "    f = (torch.log(torch.as_tensor(1000)) / (l - 1)).to(device)\n",
    "    mult = torch.arange(0, l, 1).to(device)\n",
    "    freqs = mult * f\n",
    "\n",
    "    freqs = freqs.unsqueeze(0) # (1, l)\n",
    "    x = x.unsqueeze(-1) # (B, 1, 1)\n",
    "    \n",
    "    sin_embedding = torch.sin(2* torch.pi * torch.exp(freqs) * x)\n",
    "    cos_embedding = torch.cos(2* torch.pi * torch.exp(freqs) * x)\n",
    "\n",
    "    embedding = torch.cat([sin_embedding, cos_embedding], dim=-1)\n",
    "  \n",
    "    \n",
    "    return embedding.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.ones((10, 1, 1, 1)) * 0.1\n",
    "embedding = sinusoidal_embedding(noise, 16)\n",
    "print(embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in torch.arange(0, 1, 0.1):\n",
    "    embedding_x = sinusoidal_embedding(x, 16)[0]\n",
    "    plt.plot(embedding_x, label=str(f\"{x.item():.1f}\"))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"embedding dimension\")\n",
    "plt.ylabel(\"embedding value\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding_log(x, l):\n",
    "    embedding = []\n",
    "    freqs = torch.exp(torch.linspace(torch.log(torch.as_tensor(1.0)), \n",
    "                                     torch.log(torch.as_tensor(1000.0)),\n",
    "                                     l))\n",
    "    \n",
    "    freqs = freqs.unsqueeze(0)\n",
    "    x = x.unsqueeze(-1)\n",
    "    \n",
    "    sin_embeddings = torch.sin(2* torch.pi * freqs * x)\n",
    "    cos_embeddings =torch.cos(2* torch.pi * freqs * x)\n",
    "\n",
    "    embedding = torch.cat([sin_embeddings, cos_embeddings], dim=-1)\n",
    "    \n",
    "    return embedding.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in torch.arange(0, 1, 0.1):\n",
    "    embedding_x = sinusoidal_embedding_log(x, 16)[0]\n",
    "    plt.plot(embedding_x, label=str(f\"{x.item():.1f}\"))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"embedding dimension\")\n",
    "plt.ylabel(\"embedding value\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "for y in torch.arange(0, 1, 0.01):\n",
    "    embedding_list.append(sinusoidal_embedding(y, NOISE_EMBEDDING_SIZE/2)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = torch.as_tensor((torch.stack(embedding_list).transpose(0, 1)))\n",
    "labels = [f\"{value.item():.1f}\" for value in torch.arange(0.0, 1.0, 0.1)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(\n",
    "    torch.arange(0, 100, 10), labels=labels\n",
    ")\n",
    "ax.set_ylabel(\"embedding dimension\", fontsize=8)\n",
    "ax.set_xlabel(\"noise variance\", fontsize=8)\n",
    "plt.pcolor(embedding_array, cmap=\"coolwarm\")\n",
    "plt.colorbar(orientation=\"horizontal\", label=\"embedding value\")\n",
    "ax.imshow(embedding_array, interpolation=\"nearest\", origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.arange(-4, 4, 0.1)\n",
    "\n",
    "plt.plot(sample_input, F.sigmoid(sample_input), label=\"sigmoid\")\n",
    "plt.plot(sample_input, swish(sample_input), linewidth=3, label=\"swish\")\n",
    "plt.plot(sample_input, F.silu(sample_input), label=\"silu\")\n",
    "plt.plot(sample_input, F.relu(sample_input), label=\"relu\")\n",
    "plt.xlabel(\"input value\")\n",
    "plt.ylabel(\"activation function output\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResdualBlock(Module):\n",
    "    def __init__(self, input_channels, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "        # define the layers\n",
    "        self.residual_conv = nn.Conv2d(self.input_channels, self.channels, \n",
    "                                       kernel_size=1, stride=1, padding=\"same\")\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(self.input_channels, affine=False)\n",
    "        self.conv1 = nn.Conv2d(self.input_channels, self.channels, kernel_size=3, \n",
    "                               stride=1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(self.channels, self.channels, kernel_size=3, \n",
    "                               stride=1, padding=\"same\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = x.shape[1]\n",
    "\n",
    "        assert c == self.input_channels\n",
    "\n",
    "        # check if we need to increase the number of channels\n",
    "        if self.channels == c:\n",
    "            resduial = x\n",
    "        else:\n",
    "            resduial = self.residual_conv(x)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.selu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + resduial\n",
    "\n",
    "        return x\n",
    "    \n",
    "class DownBlock(Module):\n",
    "    def __init__(self, input_channels, channels, block_depth):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.input_channels = input_channels\n",
    "        self.block_depth = block_depth\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        self.avrg_pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        for i in range(self.block_depth):\n",
    "            # just for the first one we will use the input channel size\n",
    "            if i == 0:\n",
    "                res_block = ResdualBlock(self.input_channels, self.channels)\n",
    "            else:\n",
    "                res_block = ResdualBlock(self.channels, self.channels)\n",
    "            self.residual_blocks.append(res_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skips = x\n",
    "\n",
    "        for res_block in self.residual_blocks:\n",
    "            x = res_block(x)\n",
    "            # store the output in the skip\n",
    "            skips.append(x)\n",
    "        x = self.avrg_pool(x)\n",
    "        return x, skips\n",
    "\n",
    "class upBlock(Module):\n",
    "    def __init__(self, input_channels, channels, resdual_channels, block_depth):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.block_depth = block_depth\n",
    "        self.resdual_channels = resdual_channels\n",
    "        self.input_channels = input_channels\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        for i in range(self.block_depth):\n",
    "            if i == 0:\n",
    "                res_block = ResdualBlock(self.input_channels + self.resdual_channels, self.channels)\n",
    "            else:\n",
    "                res_block = ResdualBlock(self.channels + self.resdual_channels, self.channels)\n",
    "            self.residual_blocks.append(res_block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, skips = x\n",
    "        # print(f\"upblock x: {x.shape}\")\n",
    "        x = self.upsample(x)\n",
    "        # print(f\"upblock x2: {x.shape}\")\n",
    "\n",
    "        for res_block in self.residual_blocks:\n",
    "            x = torch.cat([x, skips.pop()], dim=1)\n",
    "            # print(f\"upblock cat: {x.shape}\")\n",
    "            x = res_block(x)\n",
    "\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Module):\n",
    "    def __init__(self, device, noise_embedding_size=32):\n",
    "        super().__init__()\n",
    "        self.noise_embedding_size = noise_embedding_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.noise_upsampling = nn.Upsample(size = IMAGE_SIZE)\n",
    "        self.conv_input = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=1, \n",
    "                                    stride=1, padding=\"same\")\n",
    "\n",
    "        self.down_block_1 = DownBlock(input_channels=64, channels=32, block_depth=2)\n",
    "        self.down_block_2 = DownBlock(input_channels=32, channels=64, block_depth=2)\n",
    "        self.down_block_3 = DownBlock(input_channels=64, channels=96, block_depth=2)\n",
    "\n",
    "        self.resdual_block_1 = ResdualBlock(input_channels=96, channels=128)\n",
    "        self.resdual_block_2 = ResdualBlock(input_channels=128, channels=128)\n",
    "\n",
    "        self.up_block_1 = upBlock(input_channels=128, channels=96, resdual_channels=96, block_depth=2)\n",
    "        self.up_block_2 = upBlock(input_channels=96, channels=64, resdual_channels=64, block_depth=2)\n",
    "        self.up_block_3 = upBlock(input_channels=64, channels=32, resdual_channels=32, block_depth=2)\n",
    "\n",
    "        self.conv_output = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1, \n",
    "                                     stride=1, padding=\"same\")\n",
    "        \n",
    "        init.zeros_(self.conv_output.weight)\n",
    "    \n",
    "    def forward(self, noise_var, noisy_image):\n",
    "        # noise_var, noisy_image = x\n",
    "\n",
    "        # print(f\"noise_var: {noise_var.shape}\")\n",
    "        B = noise_var.shape[0]\n",
    "        noise_emb = sinusoidal_embedding(noise_var, self.noise_embedding_size / 2, device=self.device)\n",
    "\n",
    "        # print(f\"noise_emb: {noise_emb.shape}\")\n",
    "        noise_channels = noise_emb.shape[1]\n",
    "\n",
    "        noise_emb = noise_emb.unsqueeze(-1).unsqueeze(-1)\n",
    "  \n",
    "        \n",
    "        # print(f\"noise_emb 1: {noise_emb.shape}\")\n",
    "        noise_emb = self.noise_upsampling(noise_emb)\n",
    "        # print(f\"noise_emb 2: {noise_emb.shape}\")\n",
    "\n",
    "        # print(f\"noisy_image 1: {noisy_image.shape}\")\n",
    "\n",
    "        noisy_image = self.conv_input(noisy_image)\n",
    "\n",
    "        # print(f\"noisy_image 2: {noisy_image.shape}\")\n",
    "\n",
    "        # x = torch.cat([noise_emb, noisy_image], dim=1)\n",
    "        x = torch.cat([noisy_image, noise_emb], dim=1)\n",
    "\n",
    "        # print(f\"x: {x.shape}\")\n",
    "\n",
    "        skips = []\n",
    "        x, skips = self.down_block_1((x, skips))\n",
    "        x, skips = self.down_block_2((x, skips))\n",
    "        x, skips = self.down_block_3((x, skips))\n",
    "\n",
    "        x = self.resdual_block_1(x)\n",
    "        x = self.resdual_block_2(x)\n",
    "\n",
    "        x, skips = self.up_block_1((x, skips))\n",
    "        x, skips = self.up_block_2((x, skips))\n",
    "        x, skips = self.up_block_3((x, skips))\n",
    "\n",
    "        x = self.conv_output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(noise_embedding_size=NOISE_EMBEDDING_SIZE, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(unet, [(1,1), (1, 3, IMAGE_SIZE, IMAGE_SIZE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = sample_images.shape[0]\n",
    "noise_var = (torch.ones((B, 1)) * 0.1).to(device)\n",
    "pred_noise = unet(noise_var, sample_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_noise.shape)\n",
    "display(pred_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(Module):\n",
    "    def __init__(self, diffusion_schdule, device, \n",
    "                 noise_embedding_size=32, ema_momentum=0.999, log_dir=\"./\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.diffusion_schdule = diffusion_schdule\n",
    "        self. device = device\n",
    "        self.noise_embedding_size = noise_embedding_size\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "        self.model = UNet(noise_embedding_size=self.noise_embedding_size, device=self.device)\n",
    "        \n",
    "        # self.ema_model = UNet(noise_embedding_size=self.noise_embedding_size, device=self.device)\n",
    "        # Initialize all the parameters similar to the model\n",
    "        # self.ema_model.load_state_dict(self.model.state_dict())\n",
    "        self.ema_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "    \n",
    "    def normalize_images(self, images):\n",
    "        mean = torch.mean(images)\n",
    "        std = torch.std(images)\n",
    "        images_norm = (images - mean) / std\n",
    "        return images_norm\n",
    "    \n",
    "    def de_normalize_images(self, images):\n",
    "        denorm_images = (images * self.data_std) + self.data_mean\n",
    "        denorm_images = torch.clamp(denorm_images, min=0, max=1)\n",
    "\n",
    "        return denorm_images\n",
    "    \n",
    "    def compute_global_stat(self, data_loader):\n",
    "        self.data_mean = 0\n",
    "        self.data_std = 0\n",
    "\n",
    "        dataset = torch.cat([batch for batch, _ in data_loader])\n",
    "        self.data_mean = torch.mean(dataset)\n",
    "        self.data_std = torch.std(dataset)\n",
    "        # for data, _ in data_loader:\n",
    "        #     self.data_mean += torch.mean(data)\n",
    "        #     self.data_std += torch.std(data)\n",
    "        \n",
    "        # self.data_mean /= len(data_loader)\n",
    "        # self.data_std /= len(data_loader)\n",
    "\n",
    "    # to be used for loading checkpoints\n",
    "    def set_global_stat(self, data_mean, data_std):\n",
    "        self.data_mean = data_mean\n",
    "        self.data_std = data_std\n",
    "        \n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, training=True):\n",
    "        \n",
    "        if training:\n",
    "            pred_noise = self.model(noise_rates.squeeze(1).squeeze(1)**2, noisy_images)\n",
    "        else:\n",
    "            pred_noise = self.ema_model(noise_rates.squeeze(1).squeeze(1)**2, noisy_images)\n",
    "        \n",
    "        pred_images = (noisy_images - (pred_noise * noise_rates)) / signal_rates\n",
    "\n",
    "        return pred_noise, pred_images\n",
    "    \n",
    "    def train_step(self, train_images):\n",
    "        \n",
    "        train_images = train_images.to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        # normalize the images\n",
    "        norm_images = self.normalize_images(train_images)\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(norm_images).to(self.device)\n",
    "        # generate random diffusion times\n",
    "        B = train_images.shape[0]\n",
    "        diffusion_times = torch.rand((B, 1, 1, 1)).to(self.device)\n",
    "        # calculate the noise and signal rates\n",
    "        noise_rates, signal_rates = self.diffusion_schdule(diffusion_times)\n",
    "        # calculate noisey images\n",
    "        # print(f\"norm_images: {norm_images.shape}\")\n",
    "        # print(f\"signal_rates: {signal_rates.shape}\")\n",
    "        # print(f\"noise: {noise.shape}\")\n",
    "        # print(f\"noise_rates: {noise_rates.shape}\")\n",
    "        \n",
    "        noisy_images = (norm_images * signal_rates) + (noise * noise_rates)\n",
    "        # predict the noise and get the corretced image\n",
    "        pred_noise, pred_images = self.denoise(noisy_images, noise_rates, signal_rates, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_fucntion(pred_noise, noise)\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        #update the weights for model\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update the weights for EMA model\n",
    "        for ema_parameter, model_parameter in zip (self.ema_model.parameters(), self.model.parameters()):\n",
    "            # we will use the inplace operation for the tensor to avoid allocating a new tensor for the results\n",
    "            ema_parameter.data.mul_(self.ema_momentum).add_((1 - self.ema_momentum) * model_parameter.data)\n",
    "            # The following would work too but will allocate a tensor for the results\n",
    "            # ema_parameter.data = (self.ema_momentum * ema_parameter.data) + ((1 - self.ema_momentum) * model_parameter.data)\n",
    "\n",
    "        for ema_buffer, buffer in zip(self.ema_model.buffers(), self.model.buffers()):\n",
    "            data_type = ema_buffer.dtype\n",
    "            if data_type == torch.float32:\n",
    "                ema_buffer.data.mul_(self.ema_momentum).add_((1 - self.ema_momentum) * buffer.data).to(data_type)\n",
    "            else:\n",
    "                ema_buffer.copy_(buffer)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def fit(self, train_dataset, optimizer, loss_function, epochs, callbacks=[]):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fucntion = loss_function\n",
    "\n",
    "        # compute the global stat to be used later for image generation\n",
    "        self.compute_global_stat(train_dataset)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            acc_loss = 0\n",
    "\n",
    "            self.model.train()\n",
    "            self.ema_model.train()\n",
    "\n",
    "            for train_images, _ in train_dataset:\n",
    "                loss = self.train_step(train_images)\n",
    "\n",
    "                acc_loss += loss \n",
    "            \n",
    "            acc_loss /= len(train_dataset)\n",
    "\n",
    "            # print epoch progress\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs}: Training: loss: {acc_loss :.4f} \"\n",
    "            )\n",
    "\n",
    "            self.writer.add_scalar(\"loss\", acc_loss, global_step=epoch)\n",
    "\n",
    "            if callbacks:\n",
    "                logs = {\"epoch\": epoch,\n",
    "                       \"model_state_dict\":self.model.state_dict(),\n",
    "                       \"ema_model_state_dict\": self.ema_model.state_dict(),\n",
    "                       \"data_mean\": self.data_mean,\n",
    "                       \"data_std\": self.data_std,\n",
    "                       \"loss\": acc_loss,\n",
    "                       \"model\": self}\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs)\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps, use_ema_model=True):\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "        current_images = initial_noise.to(self.device)\n",
    "        B = initial_noise.shape[0]\n",
    "        if use_ema_model:\n",
    "            training_flag = False\n",
    "            self.ema_model.eval()\n",
    "        else:\n",
    "            training_flag = True\n",
    "            self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step in range(diffusion_steps):\n",
    "                curr_diffusion_time = torch.ones((B, 1, 1, 1)).to(self.device) * (1 - (step * step_size))\n",
    "                curr_noise_rate, curr_signal_rate = self.diffusion_schdule(curr_diffusion_time)\n",
    "                \n",
    "                pred_noise, pred_image = self.denoise(current_images, curr_noise_rate, \n",
    "                                                    curr_signal_rate, training=training_flag)\n",
    "                \n",
    "                next_diffusion_time = curr_diffusion_time - step_size\n",
    "                next_noise_rate, next_signal_rate = self.diffusion_schdule(next_diffusion_time)\n",
    "\n",
    "                current_images = (pred_image * next_signal_rate) + (pred_noise * next_noise_rate)\n",
    "            \n",
    "        return pred_image\n",
    "    \n",
    "    def generate_images(self, image_num, diffusion_steps, use_ema_model=True):\n",
    "        noise = torch.randn((image_num, 3, IMAGE_SIZE, IMAGE_SIZE)).to(self.device)\n",
    "        gen_images = self.reverse_diffusion(noise, diffusion_steps, use_ema_model=use_ema_model)\n",
    "        gen_images = self.de_normalize_images(gen_images)\n",
    "\n",
    "        return gen_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                          \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                          \"ema_model_state_dict\":logs[\"ema_model_state_dict\"],\n",
    "                          \"data_mean\": logs[\"data_mean\"],\n",
    "                          \"data_std\": logs[\"data_std\"],\n",
    "                          \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(Callback):\n",
    "    def __init__(self, save_dir, num_images, diff_steps, use_ema_model=True):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.num_images = num_images\n",
    "        self.diff_steps = diff_steps\n",
    "        self.use_ema_model = use_ema_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        diff_model = logs[\"model\"]\n",
    "\n",
    "        gen_images = diff_model.generate_images(self.num_images, self.diff_steps, self.use_ema_model)\n",
    "\n",
    "        display(gen_images, save_to=self.save_dir + f\"/epoch_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [SaveCheckpoint(save_dir=checkpoint_dir, save_every=10),\n",
    "             ImageGenerator(save_dir=sample_dir, num_images=10, diff_steps=30, use_ema_model=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model = DiffusionModel(offset_cosine_schduler, device, log_dir=log_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_600.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    diff_model.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    diff_model.ema_model.load_state_dict(checkpoint[\"ema_model_state_dict\"])\n",
    "    diff_model.set_global_stat(checkpoint[\"data_mean\"], checkpoint[\"data_std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use mean abslout error\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.AdamW(diff_model.model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for buffer in diff_model.ema_model.buffers():\n",
    "    print(type(buffer))\n",
    "    print(buffer.shape)\n",
    "    print(buffer.dtype)\n",
    "    print(buffer.data.dtype)\n",
    "    print(buffer)\n",
    "    print(buffer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model.fit(train_dataset, optimizer=optimizer, loss_function=loss_fn, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
