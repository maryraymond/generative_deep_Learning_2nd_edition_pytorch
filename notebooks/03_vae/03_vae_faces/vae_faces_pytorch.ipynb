{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤ª Variational Autoencoders - CelebA Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\"\n",
    "exp_dir = working_dir + \"/notebooks/03_vae/03_vae_faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)\n",
    "\n",
    "utils_path = os.path.abspath(exp_dir)\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "from notebooks.utils import display\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import PIL\n",
    "import pandas as pd\n",
    "\n",
    "from vae_utils import get_vector_from_label, add_vector_to_images, morph_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "BETA = 2000\n",
    "LOAD_MODEL = False\n",
    "TRAIN_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/celeba-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Padding (left, top, right, bottom)\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "test_size = int(len(dataset) - train_size)\n",
    "\n",
    "train_data, test_data =  torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training data size= {len(train_data)}\")\n",
    "print(f\"test data size= {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "images, lables = next(dataiter)\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[:10])\n",
    "print(lables[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the variational autoencoder <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.shape[0]\n",
    "        dim = z_mean.shape[1]\n",
    "        device = z_mean.device\n",
    "        epsilon = torch.randn(size=(batch, dim)).to(device)\n",
    "        sample = z_mean + torch.exp(0.5* z_log_var) * epsilon\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, channels, feature_num):\n",
    "        super().__init__()\n",
    "\n",
    "        p = self._get_padding_size(image_size, 2, 3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=feature_num, kernel_size=3, stride=2, padding=p)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=feature_num)\n",
    "        \n",
    "        p = self._get_padding_size(image_size/2, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3, stride=2, padding=p)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=feature_num)\n",
    "\n",
    "        p = self._get_padding_size(image_size/4, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3, stride=2, padding=p)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=feature_num)\n",
    "\n",
    "        p = self._get_padding_size(image_size/8, 2, 3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3, stride=2, padding=p)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=feature_num)\n",
    "\n",
    "        self.shape_before_flattening = (feature_num, image_size/16, image_size/16)\n",
    "\n",
    "        self.fc_mean = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=Z_DIM)\n",
    "        self.fc_log_var = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=Z_DIM)\n",
    "\n",
    "        self.sampling_layer = Sampling()\n",
    "\n",
    "    def get_shape_before_flattening(self):\n",
    "        return tuple(map(int, self.shape_before_flattening))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        # # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_mean(x)\n",
    "\n",
    "        z = self.sampling_layer((z_mean, z_log_var))\n",
    "        \n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "endocde = Encoder(IMAGE_SIZE, CHANNELS, NUM_FEATURES).to(device)\n",
    "\n",
    "print(endocde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(endocde, (1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shape_before_flatten, channels, feature_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape_before_flatten = shape_before_flatten\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=Z_DIM, out_features=int(math.prod(self.shape_before_flatten)))\n",
    "        self.bn_fc = nn.BatchNorm1d(num_features=int(math.prod(self.shape_before_flatten)))\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1], stride=2, kernaal_size=3)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=self.shape_before_flatten[0], out_channels=feature_num, \n",
    "                                              kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=feature_num)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*2, stride=2, kernaal_size=3)\n",
    "\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3, \n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=feature_num)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*4, stride=2, kernaal_size=3)\n",
    "        self.conv_trans3 = nn.ConvTranspose2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=feature_num)\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*8, stride=2, kernaal_size=3)\n",
    "        self.conv_trans4 = nn.ConvTranspose2d(in_channels=feature_num, out_channels=feature_num, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=feature_num)\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*16, stride=1, kernaal_size=3)\n",
    "        self.conv_trans5 = nn.ConvTranspose2d(in_channels=feature_num, out_channels=channels, \n",
    "                                              kernel_size=3, stride=1, padding=1, output_padding=0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernaal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernaal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.ceil(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        c, w, h = self.shape_before_flatten\n",
    "        x = x.view(-1, c, w, h)\n",
    "        x = self.conv_trans1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_trans2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_trans3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_trans4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_trans5(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # Should we add sigmoid?\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_before_faltten = endocde.get_shape_before_flattening()\n",
    "decoder = Decoder(shape_before_faltten, CHANNELS, NUM_FEATURES).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(decoder, (1, Z_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class var_autoencoder(nn.Module):\n",
    "    def __init__(self, image_size, channels, feature_num, log_dir=\"./log\"):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(image_size, channels, feature_num)\n",
    "        shape_before_flatten = self.encoder.get_shape_before_flattening()\n",
    "        self.decoder = Decoder(shape_before_flatten, channels, feature_num)\n",
    "\n",
    "        # tensorboard writer\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "        self.writer_val = SummaryWriter(log_dir + \"/val\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        images = self.decoder(z)\n",
    "\n",
    "        return z_mean, z_log_var, images\n",
    "    \n",
    "    def train_step(self, train_data_loader, optimizer):\n",
    "        # training\n",
    "        self.train()\n",
    "        acc_loss, acc_rec_loss, acc_var_loss = 0, 0, 0\n",
    "\n",
    "\n",
    "        for x_train, _ in train_data_loader:\n",
    "            x_train = x_train.to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_mean, z_log_var, rec_images = self.forward(x_train)\n",
    "\n",
    "            rec_loss = self.rec_loss_fn(rec_images, x_train)\n",
    "            var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "            total_loss = (rec_loss * self.beta) + var_loss\n",
    "            # total_loss = 100 *rec_loss + 0.002 * var_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += total_loss.item()\n",
    "            acc_rec_loss += rec_loss.item()\n",
    "            acc_var_loss += var_loss.item()\n",
    "        \n",
    "        return({\"rec_loss\": acc_rec_loss/len(train_data_loader), \n",
    "                \"var_loss\": acc_var_loss/len(train_data_loader), \n",
    "                \"total_loss\": acc_loss/len(train_data_loader)})\n",
    "            \n",
    "    def eval_step(self, test_data_loader):\n",
    "        # evaluation\n",
    "        self.eval()\n",
    "\n",
    "        eval_acc_loss, eval_acc_rec_loss, eval_acc_var_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_test, _ in test_data_loader:\n",
    "                x_test = x_test.to(self.device)\n",
    "\n",
    "                z_mean, z_log_var, rec_images = self.forward(x_test)\n",
    "\n",
    "                \n",
    "                eval_rec_loss = self.rec_loss_fn(rec_images, x_test)\n",
    "                eval_var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "                eval_total_loss = (eval_rec_loss * self.beta) + eval_var_loss\n",
    "                # eval_total_loss = 100 *eval_rec_loss + 0.002 * eval_var_loss\n",
    "                \n",
    "                eval_acc_rec_loss += eval_rec_loss.item()\n",
    "                eval_acc_var_loss += eval_var_loss.item()\n",
    "                eval_acc_loss += eval_total_loss.item()\n",
    "\n",
    "        return({\"rec_loss\": eval_acc_rec_loss/len(test_data_loader), \n",
    "                \"var_loss\": eval_acc_var_loss/len(test_data_loader), \n",
    "                \"total_loss\": eval_acc_loss/len(test_data_loader)})\n",
    "\n",
    "    def save_checkpoint(self, optimizer, epoch, loss, checkpoint_dir):\n",
    "        checkpoint = {\n",
    "            \"epoch\":epoch,\n",
    "            \"model_state_dict\":self.state_dict(),\n",
    "            \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            \"loss\":loss\n",
    "        }\n",
    "        checkpoint_file = checkpoint_dir + f\"/checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "\n",
    "    def fit(self, train_data_loader, test_data_loader, optimizer,\n",
    "            rec_loss_fn, var_loss_fn, device, epochs=10, beta=500, checkpoint_dir=\"./checkpoint\"):\n",
    "        \n",
    "        self.rec_loss_fn = rec_loss_fn\n",
    "        self.var_loss_fn = var_loss_fn\n",
    "        self.device = device\n",
    "        self.beta = beta\n",
    "        \n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_losses = self.train_step(train_data_loader, optimizer)\n",
    "\n",
    "            eval_losses = self.eval_step(test_data_loader)\n",
    "\n",
    "            \n",
    "            # log to tensor board\n",
    "            # self.writer.add_scalar('rec_loss', {'train': train_losses['rec_loss'],\n",
    "            #                                     'val':eval_losses['rec_loss'] }, global_step=i)\n",
    "            self.writer_train.add_scalar(\"rec_loss\",train_losses['rec_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"kl_loss\",train_losses['var_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"total_loss\",train_losses['total_loss'], global_step=i)\n",
    "            \n",
    "            self.writer_val.add_scalar(\"rec_loss\", eval_losses['rec_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"kl_loss\", eval_losses['var_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"total_loss\", eval_losses['total_loss'], global_step=i)\n",
    "\n",
    "\n",
    "            # save checkpoint\n",
    "            self.save_checkpoint(optimizer, i, train_losses, checkpoint_dir)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: rec_loss: {train_losses['rec_loss'] :.4f} \"\n",
    "                f\"var_loss: {train_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {train_losses['total_loss']:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Evaluation: rec_loss: {eval_losses['rec_loss']:.4f} \"\n",
    "                f\"var_loss: {eval_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {eval_losses['total_loss'] :.4f}\"\n",
    "            )\n",
    "            \n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "vae = var_autoencoder(IMAGE_SIZE, CHANNELS, NUM_FEATURES, log_dir=log_dir).to(device)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mean, z_log_var):\n",
    "    B = z_mean.shape[0]\n",
    "    loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    # get the mean loss\n",
    "    loss = loss / B\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the variational autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss_fn =  nn.BCELoss()\n",
    "var_loss_fn = kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir =  exp_dir + \"/checkpoint\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "vae.fit(train_data_loader=train_data_loader, test_data_loader=test_data_loader,\n",
    "        optimizer=optimizer, rec_loss_fn=reconstruction_loss_fn, var_loss_fn=var_loss_fn, \n",
    "        device=device, epochs=EPOCHS, beta=BETA, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained models\n",
    "model_dir = exp_dir + \"/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.save(vae.encoder.state_dict(), model_dir + \"/encoder\")\n",
    "torch.save(vae.decoder.state_dict(), model_dir+\"/decoder\")\n",
    "torch.save(vae.state_dict(), model_dir+\"/vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct using the variational autoencoder <a name=\"reconstruct\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 5000\n",
    "print(len(test_data))\n",
    "test_iter = iter(test_data_loader)\n",
    "example_images, example_lables = [], []\n",
    "\n",
    "while (len(example_images)*BATCH_SIZE) < n_to_predict:\n",
    "    test_images_batch, test_lables_batch = next(test_iter)\n",
    "    example_images.append(test_images_batch)\n",
    "    example_lables.append(test_lables_batch)\n",
    "\n",
    "example_images = torch.stack(example_images)\n",
    "w, h, c = example_images.shape[2:]\n",
    "example_images = example_images.view(-1, w, h, c)\n",
    "\n",
    "example_lables = torch.stack(example_lables).view(-1)\n",
    "\n",
    "print(example_images.shape)\n",
    "print(example_lables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    z_mean, z_log_var, rec_images = vae.forward(example_images[0:20].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of real cloths\")\n",
    "display(example_images)\n",
    "print(\"Example reconstrcuted images\")\n",
    "display(rec_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = iter(train_data_loader)\n",
    "sample_train, _ = next(train_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    _, _, rec_example = vae.forward(sample_train[0:20].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_train)\n",
    "display(rec_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent space distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    z_mean, z_log_var, z = vae.encoder(example_images.to(device))\n",
    "\n",
    "embeddings_np = z.to(\"cpu\").detach().numpy()\n",
    "print(embeddings_np[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "\n",
    "for i in range(50):\n",
    "    ax = fig.add_subplot(5, 10, i + 1)\n",
    "    ax.hist(embeddings_np[:, i], density=True, bins=20)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.5, -0.35, str(i), fontsize=10, ha=\"center\", transform=ax.transAxes\n",
    "    )\n",
    "    ax.plot(x, norm.pdf(x))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate new faces <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some points in the latent space, from the standard normal distribution\n",
    "grid_width, grid_height = (10, 3)\n",
    "z_sample = torch.randn(size=(grid_width * grid_height, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sampled points\n",
    "type = example_images.dtype\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    reconstructions = vae.decoder.forward(z_sample.to(device).to(type)).permute(0, 2, 3,1).to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manipulate the images <a name=\"manipulate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a data loader with custom labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = dataset_dir + \"/img_align_celeba/img_align_celeba/\"\n",
    "labels_file = dataset_dir + \"/list_attr_celeba.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWLabelsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images_dir, images_list, labels_list, transforms):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels = labels_list\n",
    "        self.transforms = transforms\n",
    "        self.img_files = images_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(self.images_dir + self.img_files[idx]).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return(image, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(labels_file)\n",
    "print(attributes.columns)\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"Blond_Hair\"\n",
    "IMAGE_HEAD = \"image_id\"\n",
    "attributes[LABEL].head()\n",
    "\n",
    "image_list = attributes[IMAGE_HEAD].to_list()\n",
    "labels_list = attributes[LABEL].to_list()\n",
    "\n",
    "print(len(image_list), len(labels_list))\n",
    "print(image_list[0], labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), \n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "dataset_wlabels = ImageWLabelsDataset(images_dir=image_dir, images_list=image_list, \n",
    "                              labels_list=labels_list, transforms=transform)\n",
    "\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "test_size = int(len(dataset) - train_size)\n",
    "\n",
    "train_data_wlabels, test_data_wlabels =  torch.utils.data.random_split(dataset_wlabels, [train_size, test_size])\n",
    "\n",
    "train_data_loader_wlabels = DataLoader(train_data_wlabels, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data_loader_wlabels = DataLoader(test_data_wlabels, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the vector for the required attribuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the attribute vector\n",
    "attribute_vec = get_vector_from_label(train_data_loader_wlabels, vae, Z_DIM, device=device, type=type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vector_to_images(train_data_loader_wlabels, vae, feature_vec=attribute_vec , device=device, type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_faces(train_data_loader_wlabels, vae, device=device, type=type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
