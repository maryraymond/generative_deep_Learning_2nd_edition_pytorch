{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘– Variational Autoencoders - Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\"\n",
    "exp_dir = working_dir + \"/notebooks/03_vae/02_vae_fashion/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from notebooks.utils import display\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 50\n",
    "BETA = 7000\n",
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad((2, 2, 2, 2)), # Padding (left, top, right, bottom)\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(data_dir, train=True, transform=transform, download=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training data size= {len(train_data)}\")\n",
    "print(f\"test data size= {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "images, lables = next(dataiter)\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[:10])\n",
    "print(lables[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the variational autoencoder <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.shape[0]\n",
    "        dim = z_mean.shape[1]\n",
    "        device = z_mean.device\n",
    "        epsilon = torch.randn(size=(batch, dim)).to(device)\n",
    "        sample = z_mean + torch.exp(0.5* z_log_var) * epsilon\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        p = self._get_padding_size(image_size, 2, 3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=3, stride=2, padding=p)\n",
    "        \n",
    "        p = self._get_padding_size(image_size/2, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        p = self._get_padding_size(image_size/4, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        self.shape_before_flattening = (128, image_size/8, image_size/8)\n",
    "\n",
    "        self.fc_mean = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "        self.fc_log_var = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "\n",
    "        self.sampling_layer = Sampling()\n",
    "\n",
    "    def get_shape_before_flattening(self):\n",
    "        return tuple(map(int, self.shape_before_flattening))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_mean(x)\n",
    "\n",
    "        z = self.sampling_layer((z_mean, z_log_var))\n",
    "        \n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "endocde = Encoder(IMAGE_SIZE, CHANNELS).to(device)\n",
    "\n",
    "print(endocde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(endocde, (1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shape_before_flatten, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape_before_flatten = shape_before_flatten\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=EMBEDDING_DIM, out_features=int(math.prod(self.shape_before_flatten)))\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1], stride=2, kernaal_size=3)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=self.shape_before_flatten[0], out_channels=128, \n",
    "                                              kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*2, stride=2, kernaal_size=3)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, \n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*4, stride=2, kernaal_size=3)\n",
    "        self.conv_trans3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=channels, kernel_size=3,\n",
    "                               stride=1, padding='same')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernaal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernaal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.ceil(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        c, w, h = self.shape_before_flatten\n",
    "        x = x.view(-1, c, w, h)\n",
    "        x = self.conv_trans1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # Should we add sigmoid?\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_before_faltten = endocde.get_shape_before_flattening()\n",
    "decoder = Decoder(shape_before_faltten, CHANNELS).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(decoder, (1, EMBEDDING_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class var_autoencoder(nn.Module):\n",
    "    def __init__(self, image_size, channels, log_dir=\"./log\"):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(image_size, channels)\n",
    "        shape_before_flatten = self.encoder.get_shape_before_flattening()\n",
    "        self.decoder = Decoder(shape_before_flatten, channels)\n",
    "\n",
    "        # tensorboard writer\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "        self.writer_val = SummaryWriter(log_dir + \"/val\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        images = self.decoder(z)\n",
    "\n",
    "        return z_mean, z_log_var, images\n",
    "    \n",
    "    def train_step(self, train_data_loader, optimizer):\n",
    "        # training\n",
    "        self.train()\n",
    "        acc_loss, acc_rec_loss, acc_var_loss = 0, 0, 0\n",
    "\n",
    "\n",
    "        for x_train, _ in train_data_loader:\n",
    "            x_train = x_train.to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_mean, z_log_var, rec_images = self.forward(x_train)\n",
    "\n",
    "            rec_loss = self.rec_loss_fn(rec_images, x_train)\n",
    "            var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "            total_loss = (rec_loss * self.beta) + var_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += total_loss.item()\n",
    "            acc_rec_loss += rec_loss.item()\n",
    "            acc_var_loss += var_loss.item()\n",
    "        \n",
    "        return({\"rec_loss\": acc_rec_loss/len(train_data_loader), \n",
    "                \"var_loss\": acc_var_loss/len(train_data_loader), \n",
    "                \"total_loss\": acc_loss/len(train_data_loader)})\n",
    "            \n",
    "    def eval_step(self, test_data_loader):\n",
    "        # evaluation\n",
    "        self.eval()\n",
    "\n",
    "        eval_acc_loss, eval_acc_rec_loss, eval_acc_var_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_test, _ in test_data_loader:\n",
    "                x_test = x_test.to(self.device)\n",
    "\n",
    "                z_mean, z_log_var, rec_images = self.forward(x_test)\n",
    "\n",
    "                \n",
    "                eval_rec_loss = self.rec_loss_fn(rec_images, x_test)\n",
    "                eval_var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "                eval_total_loss = (eval_rec_loss * self.beta) + eval_var_loss\n",
    "                \n",
    "                eval_acc_rec_loss += eval_rec_loss.item()\n",
    "                eval_acc_var_loss += eval_var_loss.item()\n",
    "                eval_acc_loss += eval_total_loss.item()\n",
    "\n",
    "        return({\"rec_loss\": eval_acc_rec_loss/len(test_data_loader), \n",
    "                \"var_loss\": eval_acc_var_loss/len(test_data_loader), \n",
    "                \"total_loss\": eval_acc_loss/len(test_data_loader)})\n",
    "\n",
    "    def save_checkpoint(self, optimizer, epoch, loss, checkpoint_dir):\n",
    "        checkpoint = {\n",
    "            \"epoch\":epoch,\n",
    "            \"model_state_dict\":self.state_dict(),\n",
    "            \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            \"loss\":loss\n",
    "        }\n",
    "        checkpoint_file = checkpoint_dir + f\"/checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "\n",
    "    def fit(self, train_data_loader, test_data_loader, optimizer,\n",
    "            rec_loss_fn, var_loss_fn, device, epochs=10, beta=500, checkpoint_dir=\"./checkpoint\"):\n",
    "        \n",
    "        self.rec_loss_fn = rec_loss_fn\n",
    "        self.var_loss_fn = var_loss_fn\n",
    "        self.device = device\n",
    "        self.beta = beta\n",
    "        \n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_losses = self.train_step(train_data_loader, optimizer)\n",
    "\n",
    "            eval_losses = self.eval_step(test_data_loader)\n",
    "\n",
    "            \n",
    "            # log to tensor board\n",
    "            # self.writer.add_scalar('rec_loss', {'train': train_losses['rec_loss'],\n",
    "            #                                     'val':eval_losses['rec_loss'] }, global_step=i)\n",
    "            self.writer_train.add_scalar(\"rec_loss\",train_losses['rec_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"kl_loss\",train_losses['var_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"total_loss\",train_losses['total_loss'], global_step=i)\n",
    "            \n",
    "            self.writer_val.add_scalar(\"rec_loss\", eval_losses['rec_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"kl_loss\", eval_losses['var_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"total_loss\", eval_losses['total_loss'], global_step=i)\n",
    "\n",
    "\n",
    "            # save checkpoint\n",
    "            self.save_checkpoint(optimizer, i, train_losses, checkpoint_dir)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: rec_loss: {train_losses['rec_loss'] :.4f} \"\n",
    "                f\"var_loss: {train_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {train_losses['total_loss']:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Evaluation: rec_loss: {eval_losses['rec_loss']:.4f} \"\n",
    "                f\"var_loss: {eval_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {eval_losses['total_loss'] :.4f}\"\n",
    "            )\n",
    "            \n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "vae = var_autoencoder(IMAGE_SIZE, CHANNELS, log_dir=log_dir).to(device)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mean, z_log_var):\n",
    "    loss = -0.5 * torch.sum(1 + z_log_var - z_mean **2 - torch.exp(z_log_var))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the variational autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss_fn =  nn.BCELoss()\n",
    "var_loss_fn = kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir =  exp_dir + \"/checkpoint\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "vae.fit(train_data_loader=train_data_loader, test_data_loader=test_data_loader,\n",
    "        optimizer=optimizer, rec_loss_fn=reconstruction_loss_fn, var_loss_fn=var_loss_fn, \n",
    "        device=device, epochs=EPOCHS, beta=BETA, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained models\n",
    "model_dir = exp_dir + \"/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.save(vae.encoder.state_dict(), model_dir + \"/encoder\")\n",
    "torch.save(vae.decoder.state_dict(), model_dir+\"/decoder\")\n",
    "torch.save(vae.state_dict(), model_dir+\"/vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct using the variational autoencoder <a name=\"reconstruct\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 5000\n",
    "print(len(test_data))\n",
    "test_iter = iter(test_data_loader)\n",
    "example_images, example_lables = [], []\n",
    "\n",
    "while (len(example_images)*BATCH_SIZE) < n_to_predict:\n",
    "    test_images_batch, test_lables_batch = next(test_iter)\n",
    "    example_images.append(test_images_batch)\n",
    "    example_lables.append(test_lables_batch)\n",
    "\n",
    "example_images = torch.stack(example_images)\n",
    "w, h, c = example_images.shape[2:]\n",
    "example_images = example_images.view(-1, w, h, c)\n",
    "\n",
    "example_lables = torch.stack(example_lables).view(-1)\n",
    "\n",
    "print(example_images.shape)\n",
    "print(example_lables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, rec_images = vae.forward(example_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of real cloths\")\n",
    "display(example_images)\n",
    "print(\"Example reconstrcuted images\")\n",
    "display(rec_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = iter(train_data_loader)\n",
    "sample_train, _ = next(train_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, rec_example = vae.forward(sample_train.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_train)\n",
    "display(rec_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed using the encoder <a name=\"encode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_mean, z_log_var, z = vae.encoder(example_images.to(device))\n",
    "\n",
    "embeddings_np = z.to(\"cpu\").detach().numpy()\n",
    "print(embeddings_np[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate using the decoder <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, maxs = np.min(embeddings_np, axis=0), np.max(embeddings_np, axis=0)\n",
    "print(mins)\n",
    "print(maxs)\n",
    "grid_width, grid_height = (6, 6)\n",
    "\n",
    "samples = np.random.uniform(mins, maxs, (grid_width*grid_height, EMBEDDING_DIM))\n",
    "\n",
    "print(samples[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = z.dtype\n",
    "with torch.no_grad():\n",
    "    reconstructions = vae.decoder(torch.as_tensor(samples).to(type).to(device))\n",
    "\n",
    "print(reconstructions.shape)\n",
    "reconstructions_np = reconstructions.permute(0, 2, 3,1).to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], c=\"blue\", alpha=1, s=20)\n",
    "    plt.show()\n",
    "\n",
    "    # Add underneath a grid of the decoded images\n",
    "    fig = plt.figure(figsize=(figure_size, grid_height * 2))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i in range(grid_width * grid_height):\n",
    "        ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.35,\n",
    "            str(np.round(samples[i, :], 1)),\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore the latent space <a name=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original embeddings and sampled embeddings to p-values\n",
    "p = norm.cdf(embeddings_np)\n",
    "p_sample = norm.cdf(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 8\n",
    "fig = plt.figure(figsize=(figsize * 2, figsize))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plot_1 = ax.scatter(\n",
    "    embeddings_np[:, 0], embeddings_np[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=3\n",
    ")\n",
    "plt.colorbar(plot_1)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plot_2 = ax.scatter(\n",
    "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 12\n",
    "grid_size = 15\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(\n",
    "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=300\n",
    ")\n",
    "plt.colorbar()\n",
    "\n",
    "x = norm.ppf(np.linspace(0, 1, grid_size))\n",
    "y = norm.ppf(np.linspace(1, 0, grid_size))\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "xv = xv.flatten()\n",
    "yv = yv.flatten()\n",
    "grid = np.array(list(zip(xv, yv)))\n",
    "\n",
    "reconstructions = vae.decoder.forward(torch.as_tensor(grid).to(device).to(type))\n",
    "reconstructions_np = reconstructions.permute(0, 2, 3,1).to(\"cpu\").detach().numpy()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(figsize, figsize))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(grid_size**2):\n",
    "    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
