{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游녰 Variational Autoencoders - Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\"\n",
    "exp_dir = working_dir + \"/notebooks/03_vae/02_vae_fashion/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from notebooks.utils import display\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 50\n",
    "BETA = 500\n",
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad((2, 2, 2, 2)), # Padding (left, top, right, bottom)\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(data_dir, train=True, transform=transform, download=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size= 60000\n",
      "test data size= 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"training data size= {len(train_data)}\")\n",
    "print(f\"test data size= {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([100, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "images, lables = next(dataiter)\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 5, 3, 9, 7, 3, 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "display(images[:10])\n",
    "print(lables[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the variational autoencoder <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.shape[0]\n",
    "        dim = z_mean.shape[1]\n",
    "        device = z_mean.device\n",
    "        epsilon = torch.randn(size=(batch, dim)).to(device)\n",
    "        sample = z_mean + torch.exp(0.5* z_log_var) * epsilon\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        p = self._get_padding_size(image_size, 2, 3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=3, stride=2, padding=p)\n",
    "        \n",
    "        p = self._get_padding_size(image_size/2, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        p = self._get_padding_size(image_size/4, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        self.shape_before_flattening = (128, image_size/8, image_size/8)\n",
    "\n",
    "        self.fc_mean = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "        self.fc_log_var = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "\n",
    "        self.sampling_layer = Sampling()\n",
    "\n",
    "    def get_shape_before_flattening(self):\n",
    "        return tuple(map(int, self.shape_before_flattening))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_mean(x)\n",
    "\n",
    "        z = self.sampling_layer((z_mean, z_log_var))\n",
    "        \n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc_mean): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  (fc_log_var): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  (sampling_layer): Sampling()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "endocde = Encoder(IMAGE_SIZE, CHANNELS).to(device)\n",
    "\n",
    "print(endocde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mary/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [1, 2]                    4,098\n",
       "較럭Conv2d: 1-1                            [1, 32, 16, 16]           320\n",
       "較럭Conv2d: 1-2                            [1, 64, 8, 8]             18,496\n",
       "較럭Conv2d: 1-3                            [1, 128, 4, 4]            73,856\n",
       "較럭Linear: 1-4                            [1, 2]                    4,098\n",
       "較럭Linear: 1-5                            [1, 2]                    (recursive)\n",
       "較럭Sampling: 1-6                          [1, 2]                    --\n",
       "==========================================================================================\n",
       "Total params: 100,868\n",
       "Trainable params: 100,868\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.46\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.11\n",
       "Params size (MB): 0.39\n",
       "Estimated Total Size (MB): 0.51\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(endocde, (1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shape_before_flatten, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape_before_flatten = shape_before_flatten\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=EMBEDDING_DIM, out_features=int(math.prod(self.shape_before_flatten)))\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1], stride=2, kernaal_size=3)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=self.shape_before_flatten[0], out_channels=128, \n",
    "                                              kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*2, stride=2, kernaal_size=3)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, \n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*4, stride=2, kernaal_size=3)\n",
    "        self.conv_trans3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=channels, kernel_size=3,\n",
    "                               stride=1, padding='same')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernaal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernaal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.ceil(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        c, w, h = self.shape_before_flatten\n",
    "        x = x.view(-1, c, w, h)\n",
    "        x = self.conv_trans1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # Should we add sigmoid?\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (fc1): Linear(in_features=2, out_features=2048, bias=True)\n",
      "  (conv_trans1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_trans2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_trans3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv1): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "shape_before_faltten = endocde.get_shape_before_flattening()\n",
    "decoder = Decoder(shape_before_faltten, CHANNELS).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [1, 1, 32, 32]            --\n",
       "較럭Linear: 1-1                            [1, 2048]                 6,144\n",
       "較럭ConvTranspose2d: 1-2                   [1, 128, 8, 8]            147,584\n",
       "較럭ConvTranspose2d: 1-3                   [1, 64, 16, 16]           73,792\n",
       "較럭ConvTranspose2d: 1-4                   [1, 32, 32, 32]           18,464\n",
       "較럭Conv2d: 1-5                            [1, 1, 32, 32]            289\n",
       "==========================================================================================\n",
       "Total params: 246,273\n",
       "Trainable params: 246,273\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 47.55\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.48\n",
       "Params size (MB): 0.99\n",
       "Estimated Total Size (MB): 1.47\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(decoder, (1, EMBEDDING_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class var_autoencoder(nn.Module):\n",
    "    def __init__(self, image_size, channels, log_dir=\"./log\"):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(image_size, channels)\n",
    "        shape_before_flatten = self.encoder.get_shape_before_flattening()\n",
    "        self.decoder = Decoder(shape_before_flatten, channels)\n",
    "\n",
    "        # tensorboard writer\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "        self.writer_val = SummaryWriter(log_dir + \"/val\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        images = self.decoder(z)\n",
    "\n",
    "        return z_mean, z_log_var, images\n",
    "    \n",
    "    def train_step(self, train_data_loader, optimizer):\n",
    "        # training\n",
    "        self.train()\n",
    "        acc_loss, acc_rec_loss, acc_var_loss = 0, 0, 0\n",
    "\n",
    "\n",
    "        for x_train, _ in train_data_loader:\n",
    "            x_train = x_train.to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_mean, z_log_var, rec_images = self.forward(x_train)\n",
    "\n",
    "            rec_loss = self.rec_loss_fn(rec_images, x_train)\n",
    "            var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "            total_loss = (rec_loss * self.beta) + var_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += total_loss.item()\n",
    "            acc_rec_loss += rec_loss.item()\n",
    "            acc_var_loss += var_loss.item()\n",
    "        \n",
    "        return({\"rec_loss\": acc_rec_loss/len(train_data_loader), \n",
    "                \"var_loss\": acc_var_loss/len(train_data_loader), \n",
    "                \"total_loss\": acc_loss/len(train_data_loader)})\n",
    "            \n",
    "    def eval_step(self, test_data_loader):\n",
    "        # evaluation\n",
    "        self.eval()\n",
    "\n",
    "        eval_acc_loss, eval_acc_rec_loss, eval_acc_var_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_test, _ in test_data_loader:\n",
    "                x_test = x_test.to(self.device)\n",
    "\n",
    "                z_mean, z_log_var, rec_images = self.forward(x_test)\n",
    "\n",
    "                \n",
    "                eval_rec_loss = self.rec_loss_fn(rec_images, x_test)\n",
    "                eval_var_loss = self.var_loss_fn(z_mean, z_log_var)\n",
    "                eval_total_loss = (eval_rec_loss * self.beta) + eval_var_loss\n",
    "                \n",
    "                eval_acc_rec_loss += eval_rec_loss.item()\n",
    "                eval_acc_var_loss += eval_var_loss.item()\n",
    "                eval_acc_loss += eval_total_loss.item()\n",
    "\n",
    "        return({\"rec_loss\": eval_acc_rec_loss/len(test_data_loader), \n",
    "                \"var_loss\": eval_acc_var_loss/len(test_data_loader), \n",
    "                \"total_loss\": eval_acc_loss/len(test_data_loader)})\n",
    "\n",
    "    def save_checkpoint(self, optimizer, epoch, loss, checkpoint_dir):\n",
    "        checkpoint = {\n",
    "            \"epoch\":epoch,\n",
    "            \"model_state_dict\":self.state_dict(),\n",
    "            \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            \"loss\":loss\n",
    "        }\n",
    "        checkpoint_file = checkpoint_dir + f\"/checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "\n",
    "    def fit(self, train_data_loader, test_data_loader, optimizer,\n",
    "            rec_loss_fn, var_loss_fn, device, epochs=10, beta=500, checkpoint_dir=\"./checkpoint\"):\n",
    "        \n",
    "        self.rec_loss_fn = rec_loss_fn\n",
    "        self.var_loss_fn = var_loss_fn\n",
    "        self.device = device\n",
    "        self.beta = beta\n",
    "        \n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_losses = self.train_step(train_data_loader, optimizer)\n",
    "\n",
    "            eval_losses = self.eval_step(test_data_loader)\n",
    "\n",
    "            \n",
    "            # log to tensor board\n",
    "            # self.writer.add_scalar('rec_loss', {'train': train_losses['rec_loss'],\n",
    "            #                                     'val':eval_losses['rec_loss'] }, global_step=i)\n",
    "            self.writer_train.add_scalar(\"rec_loss\",train_losses['rec_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"kl_loss\",train_losses['var_loss'], global_step=i)\n",
    "            self.writer_train.add_scalar(\"total_loss\",train_losses['total_loss'], global_step=i)\n",
    "            \n",
    "            self.writer_val.add_scalar(\"rec_loss\", eval_losses['rec_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"kl_loss\", eval_losses['var_loss'], global_step=i)\n",
    "            self.writer_val.add_scalar(\"total_loss\", eval_losses['total_loss'], global_step=i)\n",
    "\n",
    "\n",
    "            # save checkpoint\n",
    "            self.save_checkpoint(optimizer, i, train_losses, checkpoint_dir)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: rec_loss: {train_losses['rec_loss'] :.4f} \"\n",
    "                f\"var_loss: {train_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {train_losses['total_loss']:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Evaluation: rec_loss: {eval_losses['rec_loss']:.4f} \"\n",
    "                f\"var_loss: {eval_losses['var_loss']:.4f} \"\n",
    "                f\"total_loss: {eval_losses['total_loss'] :.4f}\"\n",
    "            )\n",
    "            \n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (fc_mean): Linear(in_features=2048, out_features=2, bias=True)\n",
      "    (fc_log_var): Linear(in_features=2048, out_features=2, bias=True)\n",
      "    (sampling_layer): Sampling()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc1): Linear(in_features=2, out_features=2048, bias=True)\n",
      "    (conv_trans1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (conv_trans2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (conv_trans3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (conv1): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "vae = var_autoencoder(IMAGE_SIZE, CHANNELS, log_dir=log_dir).to(device)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mean, z_log_var):\n",
    "    B = z_mean.shape[0]\n",
    "    loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    # get the mean loss\n",
    "    loss = loss / B\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the variational autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss_fn =  nn.BCELoss()\n",
    "var_loss_fn = kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m  exp_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_loss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreconstruction_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_loss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBETA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 90\u001b[0m, in \u001b[0;36mvar_autoencoder.fit\u001b[0;34m(self, train_data_loader, test_data_loader, optimizer, rec_loss_fn, var_loss_fn, device, epochs, beta, checkpoint_dir)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m=\u001b[39m beta\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 90\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     eval_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_step(test_data_loader)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# log to tensor board\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# self.writer.add_scalar('rec_loss', {'train': train_losses['rec_loss'],\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m#                                     'val':eval_losses['rec_loss'] }, global_step=i)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m, in \u001b[0;36mvar_autoencoder.train_step\u001b[0;34m(self, train_data_loader, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m acc_loss, acc_rec_loss, acc_var_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_train, _ \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[1;32m     25\u001b[0m     x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/multiprocessing/queues.py:116\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:110\u001b[0m, in \u001b[0;36mrebuild_tensor\u001b[0;34m(cls, storage, metadata)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_tensor\u001b[39m(\u001b[38;5;28mcls\u001b[39m, storage, metadata):\n\u001b[1;32m    109\u001b[0m     storage_offset, size, stride, requires_grad \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 110\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;66;03m# we have to pass requires_grad into constructor, rather than set it as an\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;66;03m# attribute later, because it's an important check for Integer Tensors to\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# have requires_grad=False (or else they raise an error)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter(t, requires_grad\u001b[38;5;241m=\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/nerfstudio_1.1/lib/python3.8/site-packages/torch/_utils.py:180\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir =  exp_dir + \"/checkpoint\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "vae.fit(train_data_loader=train_data_loader, test_data_loader=test_data_loader,\n",
    "        optimizer=optimizer, rec_loss_fn=reconstruction_loss_fn, var_loss_fn=var_loss_fn, \n",
    "        device=device, epochs=EPOCHS, beta=BETA, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained models\n",
    "model_dir = exp_dir + \"/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.save(vae.encoder.state_dict(), model_dir + \"/encoder\")\n",
    "torch.save(vae.decoder.state_dict(), model_dir+\"/decoder\")\n",
    "torch.save(vae.state_dict(), model_dir+\"/vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct using the variational autoencoder <a name=\"reconstruct\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 5000\n",
    "print(len(test_data))\n",
    "test_iter = iter(test_data_loader)\n",
    "example_images, example_lables = [], []\n",
    "\n",
    "while (len(example_images)*BATCH_SIZE) < n_to_predict:\n",
    "    test_images_batch, test_lables_batch = next(test_iter)\n",
    "    example_images.append(test_images_batch)\n",
    "    example_lables.append(test_lables_batch)\n",
    "\n",
    "example_images = torch.stack(example_images)\n",
    "w, h, c = example_images.shape[2:]\n",
    "example_images = example_images.view(-1, w, h, c)\n",
    "\n",
    "example_lables = torch.stack(example_lables).view(-1)\n",
    "\n",
    "print(example_images.shape)\n",
    "print(example_lables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, rec_images = vae.forward(example_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of real cloths\")\n",
    "display(example_images)\n",
    "print(\"Example reconstrcuted images\")\n",
    "display(rec_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = iter(train_data_loader)\n",
    "sample_train, _ = next(train_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, rec_example = vae.forward(sample_train.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_train)\n",
    "display(rec_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed using the encoder <a name=\"encode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_mean, z_log_var, z = vae.encoder(example_images.to(device))\n",
    "\n",
    "embeddings_np = z.to(\"cpu\").detach().numpy()\n",
    "print(embeddings_np[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate using the decoder <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, maxs = np.min(embeddings_np, axis=0), np.max(embeddings_np, axis=0)\n",
    "print(mins)\n",
    "print(maxs)\n",
    "grid_width, grid_height = (6, 6)\n",
    "\n",
    "samples = np.random.uniform(mins, maxs, (grid_width*grid_height, EMBEDDING_DIM))\n",
    "\n",
    "print(samples[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = z.dtype\n",
    "with torch.no_grad():\n",
    "    reconstructions = vae.decoder(torch.as_tensor(samples).to(type).to(device))\n",
    "\n",
    "print(reconstructions.shape)\n",
    "reconstructions_np = reconstructions.permute(0, 2, 3,1).to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], c=\"blue\", alpha=1, s=20)\n",
    "    plt.show()\n",
    "\n",
    "    # Add underneath a grid of the decoded images\n",
    "    fig = plt.figure(figsize=(figure_size, grid_height * 2))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i in range(grid_width * grid_height):\n",
    "        ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.35,\n",
    "            str(np.round(samples[i, :], 1)),\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore the latent space <a name=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original embeddings and sampled embeddings to p-values\n",
    "p = norm.cdf(embeddings_np)\n",
    "p_sample = norm.cdf(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 8\n",
    "fig = plt.figure(figsize=(figsize * 2, figsize))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plot_1 = ax.scatter(\n",
    "    embeddings_np[:, 0], embeddings_np[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=3\n",
    ")\n",
    "plt.colorbar(plot_1)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plot_2 = ax.scatter(\n",
    "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 12\n",
    "grid_size = 15\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(\n",
    "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_lables, alpha=0.8, s=300\n",
    ")\n",
    "plt.colorbar()\n",
    "\n",
    "x = norm.ppf(np.linspace(0, 1, grid_size))\n",
    "y = norm.ppf(np.linspace(1, 0, grid_size))\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "xv = xv.flatten()\n",
    "yv = yv.flatten()\n",
    "grid = np.array(list(zip(xv, yv)))\n",
    "\n",
    "reconstructions = vae.decoder.forward(torch.as_tensor(grid).to(device).to(type))\n",
    "reconstructions_np = reconstructions.permute(0, 2, 3,1).to(\"cpu\").detach().numpy()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(figsize, figsize))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(grid_size**2):\n",
    "    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
