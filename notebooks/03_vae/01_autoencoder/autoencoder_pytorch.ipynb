{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ðŸ‘– Autoencoders on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from notebooks.utils import display\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad((2, 2, 2, 2)), # Padding (left, top, right, bottom)\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(data_dir, train=True, transform=transform, download=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training data size= {len(train_data)}\")\n",
    "print(f\"test data size= {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "images, lables = next(dataiter)\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[:10])\n",
    "print(lables[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        p = self._get_padding_size(image_size, 2, 3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=3, stride=2, padding=p)\n",
    "        \n",
    "        p = self._get_padding_size(image_size/2, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        p = self._get_padding_size(image_size/4, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        self.shape_before_flattening = (128, image_size/8, image_size/8)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "\n",
    "    def get_shape_before_flattening(self):\n",
    "        return self.shape_before_flattening\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shape_before_flatten):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape_before_flatten = shape_before_flatten\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=EMBEDDING_DIM, out_features=int(math.prod(self.shape_before_flatten)))\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1], stride=2, kernaal_size=3)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=self.shape_before_flatten[0], out_channels=128, \n",
    "                                              kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*2, stride=2, kernaal_size=3)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, \n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*4, stride=2, kernaal_size=3)\n",
    "        self.conv_trans3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=CHANNELS, kernel_size=3,\n",
    "                               stride=1, padding='same')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernaal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernaal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.ceil(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        c, w, h = self.shape_before_flatten\n",
    "        x = x.view(-1, c, w, h)\n",
    "        x = self.conv_trans1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # Should we add sigmoid?\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(image_size=image_size, channels=channels)\n",
    "        self.shape_before_flatten = tuple(map(int, self.encoder.get_shape_before_flattening()))\n",
    "        self.decoder = Decoder(self.shape_before_flatten)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        img = self.decoder(emb)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(32, 1).to(device)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(encoder, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_before_flatten = encoder.get_shape_before_flattening()\n",
    "shape_before_flatten = tuple(map(int, shape_before_flatten))\n",
    "\n",
    "decoder = Decoder(shape_before_flatten).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(decoder, (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoder(32, 1).to(device)\n",
    "print(auto_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(auto_encoder, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmizer = optim.Adam(auto_encoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataloader, optimizer, loss_fn, epochs=10):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # loop over the number of epoch\n",
    "    for i in range(epochs):\n",
    "        #  set the model for training\n",
    "        model.train()\n",
    "        # loop over the dataloader to get all the data\n",
    "        running_loss = 0.0\n",
    "        num_samples = 0\n",
    "        correct = 0\n",
    "        for images, _ in train_dataloader:\n",
    "            #  zero the gradiants of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # move the training data to the same device as the model\n",
    "            images = images.to(device)\n",
    "            # Predict the lables\n",
    "            predictions = model(images)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(predictions, images)\n",
    "            # calcualte the gradients for the loss\n",
    "            loss.backward()\n",
    "            # updat the weights using the optimizer\n",
    "            optimizer.step()\n",
    "            # accumilate the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calcualte the accuracy\n",
    "            _,pred_lable = torch.max(predictions, 1)\n",
    "            # _, corr_label = torch.max(labels, 1)\n",
    "        \n",
    "        print( f\"Epoch {i} / {epochs}: loss= {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(auto_encoder, train_data_loader, optmizer, loss_fn, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_data_loader)\n",
    "test_images, test_lables = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = auto_encoder.encoder(test_images.to(device))\n",
    "gen_images = auto_encoder.decoder(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_images[:10])\n",
    "display(gen_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = auto_encoder.encoder(images.to(device))\n",
    "gen_images_train = auto_encoder.decoder(emb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[:10])\n",
    "display(gen_images_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
