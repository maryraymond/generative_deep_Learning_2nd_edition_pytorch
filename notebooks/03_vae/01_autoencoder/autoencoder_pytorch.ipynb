{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ðŸ‘– Autoencoders on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from notebooks.utils import display\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad((2, 2, 2, 2)), # Padding (left, top, right, bottom)\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(data_dir, train=True, transform=transform, download=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training data size= {len(train_data)}\")\n",
    "print(f\"test data size= {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "images, lables = next(dataiter)\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[:10])\n",
    "print(lables[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        p = self._get_padding_size(image_size, 2, 3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=3, stride=2, padding=p)\n",
    "        \n",
    "        p = self._get_padding_size(image_size/2, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        p = self._get_padding_size(image_size/4, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=p)\n",
    "\n",
    "        self.shape_before_flattening = (128, image_size/8, image_size/8)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=int(math.prod(self.shape_before_flattening)), out_features=EMBEDDING_DIM)\n",
    "\n",
    "    def get_shape_before_flattening(self):\n",
    "        return self.shape_before_flattening\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shape_before_flatten):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape_before_flatten = shape_before_flatten\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=EMBEDDING_DIM, out_features=int(math.prod(self.shape_before_flatten)))\n",
    "\n",
    "        p = self._get_padding_size(self.shape_before_flatten[1], stride=2, kernaal_size=3)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=self.shape_before_flatten[0], out_channels=128, \n",
    "                                              kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*2, stride=2, kernaal_size=3)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, \n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        p = self._get_padding_size(self.shape_before_flatten[1]*4, stride=2, kernaal_size=3)\n",
    "        self.conv_trans3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                                              stride=2, padding=p, output_padding=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=CHANNELS, kernel_size=3,\n",
    "                               stride=1, padding='same')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernaal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernaal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.ceil(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        c, w, h = self.shape_before_flatten\n",
    "        x = x.view(-1, c, w, h)\n",
    "        x = self.conv_trans1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_trans3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        # Should we add sigmoid?\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, image_size, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(image_size=image_size, channels=channels)\n",
    "        self.shape_before_flatten = tuple(map(int, self.encoder.get_shape_before_flattening()))\n",
    "        self.decoder = Decoder(self.shape_before_flatten)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        img = self.decoder(emb)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(32, 1).to(device)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(encoder, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_before_flatten = encoder.get_shape_before_flattening()\n",
    "shape_before_flatten = tuple(map(int, shape_before_flatten))\n",
    "\n",
    "decoder = Decoder(shape_before_flatten).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(decoder, (EMBEDDING_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoder(32, 1).to(device)\n",
    "print(auto_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(auto_encoder, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmizer = optim.Adam(auto_encoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataloader, optimizer, loss_fn, epochs=10):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # loop over the number of epoch\n",
    "    for i in range(epochs):\n",
    "        #  set the model for training\n",
    "        model.train()\n",
    "        # loop over the dataloader to get all the data\n",
    "        running_loss = 0.0\n",
    "        num_samples = 0\n",
    "        correct = 0\n",
    "        for images, _ in train_dataloader:\n",
    "            #  zero the gradiants of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # move the training data to the same device as the model\n",
    "            images = images.to(device)\n",
    "            # Predict the lables\n",
    "            predictions = model(images)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(predictions, images)\n",
    "            # calcualte the gradients for the loss\n",
    "            loss.backward()\n",
    "            # updat the weights using the optimizer\n",
    "            optimizer.step()\n",
    "            # accumilate the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calcualte the accuracy\n",
    "            _,pred_lable = torch.max(predictions, 1)\n",
    "            # _, corr_label = torch.max(labels, 1)\n",
    "        \n",
    "        print( f\"Epoch {i} / {epochs}: loss= {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(auto_encoder, train_data_loader, optmizer, loss_fn, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained models\n",
    "model_dir = working_dir + \"/notebooks/03_vae/01_autoencoder/models\"\n",
    "torch.save(auto_encoder.state_dict(), model_dir + \"/autoendcoder\")\n",
    "torch.save(auto_encoder.encoder.state_dict(), model_dir + \"/encoder\")\n",
    "torch.save(auto_encoder.decoder.state_dict(), model_dir +  \"/decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reconstruct using the autoencoder <a name=\"reconstruct\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 5000\n",
    "print(len(test_data))\n",
    "test_iter = iter(test_data_loader)\n",
    "example_images, example_lables = [], []\n",
    "\n",
    "while (len(example_images)*BATCH_SIZE) < n_to_predict:\n",
    "    test_images_batch, test_lables_batch = next(test_iter)\n",
    "    example_images.append(test_images_batch)\n",
    "    example_lables.append(test_lables_batch)\n",
    "\n",
    "example_images = torch.stack(example_images)\n",
    "w, h, c = example_images.shape[2:]\n",
    "example_images = example_images.view(-1, w, h, c)\n",
    "\n",
    "example_lables = torch.stack(example_lables).view(-1)\n",
    "\n",
    "print(example_images.shape)\n",
    "print(example_lables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = auto_encoder.encoder(example_images.to(device))\n",
    "    gen_images = auto_encoder.decoder(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of real items\")\n",
    "display(example_images)\n",
    "print(\"Example of reconstructed items\")\n",
    "display(gen_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embed using the encoder <a name=\"encode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = auto_encoder.encoder(example_images.to(device))\n",
    "\n",
    "embeddings_np = embeddings.to(\"cpu\").detach().numpy()\n",
    "print(embeddings_np[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    example_lables_np = example_lables.to(\"cpu\").detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1],\n",
    "                cmap=\"rainbow\",\n",
    "                c = example_lables_np,\n",
    "                alpha=0.8,\n",
    "                s=3\n",
    "                )\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate using the decoder <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, maxs = np.min(embeddings_np, axis=0), np.max(embeddings_np, axis=0)\n",
    "print(mins)\n",
    "print(maxs)\n",
    "grid_width, grid_height = (6, 6)\n",
    "\n",
    "samples = np.random.uniform(mins, maxs, (grid_width*grid_height, EMBEDDING_DIM))\n",
    "\n",
    "print(samples[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = embeddings.dtype\n",
    "with torch.no_grad():\n",
    "    reconstructions = auto_encoder.decoder(torch.as_tensor(samples).to(type).to(device))\n",
    "\n",
    "print(reconstructions.shape)\n",
    "reconstructions_np = reconstructions.permute(0, 2, 3,1).to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_DIM == 2:\n",
    "    figure_size = 8\n",
    "\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    plt.scatter(embeddings_np[:, 0], embeddings_np[:, 1], c=\"black\", s=3, alpha=0.5)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], c=\"blue\", alpha=1, s=20)\n",
    "    plt.show()\n",
    "\n",
    "    # Add underneath a grid of the decoded images\n",
    "    fig = plt.figure(figsize=(figure_size, grid_height * 2))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i in range(grid_width * grid_height):\n",
    "        ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.35,\n",
    "            str(np.round(samples[i, :], 1)),\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "if EMBEDDING_DIM == 2:\n",
    "\n",
    "    figsize = 12\n",
    "    grid_size = 15\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.scatter(\n",
    "        embeddings_np[:, 0],\n",
    "        embeddings_np[:, 1],\n",
    "        cmap=\"rainbow\",\n",
    "        c=example_lables_np,\n",
    "        alpha=0.8,\n",
    "        s=300,\n",
    "    )\n",
    "    plt.colorbar()\n",
    "\n",
    "    x = np.linspace(min(embeddings_np[:, 0]), max(embeddings_np[:, 0]), grid_size)\n",
    "    y = np.linspace(max(embeddings_np[:, 1]), min(embeddings_np[:, 1]), grid_size)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    xv = xv.flatten()\n",
    "    yv = yv.flatten()\n",
    "    grid = np.array(list(zip(xv, yv)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructions_2 = auto_encoder.decoder(torch.as_tensor(grid).to(type).to(device))\n",
    "\n",
    "    reconstructions_np_2 = reconstructions_2.permute(0, 2, 3, 1).to(\"cpu\").detach().numpy()\n",
    "    # plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(figsize, figsize))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    for i in range(grid_size**2):\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(reconstructions_np_2[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
