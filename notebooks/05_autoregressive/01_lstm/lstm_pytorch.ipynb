{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥™ LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/05_autoregressive/01_lstm) autoregressive LSTM model, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The original code is available [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition) and is licensed under the Apache License 2.0._\n",
    "_This implementation is distributed under the Apache License 2.0. See the LICENSE file for details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own LSTM on the recipes dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/05_autoregressive/01_lstm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/epirecipes\"\n",
    "data_file = dataset_dir + \"/full_format_recipes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file) as data_json:\n",
    "    data_raw = json.load(data_json)\n",
    "\n",
    "print(data_raw[0].keys())\n",
    "print(data_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [ f\"Recipe for {x['title']} | \" + \" \".join(x['directions'])\n",
    "                 for x in data_raw\n",
    "                 if \"title\" in x and\n",
    "                 x[\"title\"] is not None and\n",
    "                 \"directions\" in x and\n",
    "                 x[\"directions\"] is not None ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_data))\n",
    "print(filtered_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(str):\n",
    "    # add space before and after every punctuation\n",
    "    str = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", str)\n",
    "    # replace multiple spaces with one space\n",
    "    str = re.sub(\" +\", \" \", str)\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Hello   there!\"\n",
    "test_text = pad_punctuation(test_text)\n",
    "print(test_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = list(map(pad_punctuation, filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will set the value for the token paralization to avoid getting warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# we will use the hugging face Tokenizers package to Tokenize the dataset and create the vocab\n",
    "# We will use a simple word tokenizer\n",
    "# the tokenizer itself will handel assigning a numerical value to each word\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "# the pre tokenizer will pre process the test and split it into words (based on whitespace)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "pre_tokenized_text = tokenizer.pre_tokenizer.pre_tokenize_str(test_text)\n",
    "print(pre_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to form the vocabilary using the tokenizer we use trainer\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"<pad>\", \"<unk>\"], vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train_from_iterator(train_data_list, trainer)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "\n",
    "# enable trancation and padding for the dataste so that all entries would have the same length\n",
    "tokenizer.enable_padding(length=MAX_LEN + 1, pad_id=pad_idx, pad_token=\"<pad>\")\n",
    "tokenizer.enable_truncation(max_length=MAX_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting vocabulary\n",
    "print(\"Vocabulary size:\", tokenizer.get_vocab_size())\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"padiing index = \", pad_idx)\n",
    "vocab_idnx_to_word = {vocab[key]: key for key in vocab.keys()}\n",
    "print(vocab_idnx_to_word)\n",
    "test_vector = tokenizer.encode(test_text)\n",
    "print(test_vector.ids)\n",
    "print(test_vector.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "vectorized_data = [tokenizer.encode(sentence).ids for sentence in train_data_list]\n",
    "print(len(vectorized_data))\n",
    "print(len(vectorized_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSeqDataset(Dataset):\n",
    "    def __init__(self, vectorized_data_list):\n",
    "        super().__init__()\n",
    "        self.vectorized_data_list = vectorized_data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.vectorized_data_list))\n",
    "    \n",
    "    def get_data_pair(self, idx):\n",
    "        text = self.vectorized_data_list[idx]\n",
    "        x = torch.tensor(text[:-1])\n",
    "        y = torch.tensor(text[1:])\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_data_pair(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextSeqDataset(vectorized_data_list=vectorized_data)\n",
    "x, y = train_dataset.get_data_pair(0)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Build the LSTM <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, embedded_dim=100, lstm_units=128, \n",
    "                 pad_idx=0, is_pidirectional=False, log_dir=\"./log\"):\n",
    "        super().__init__()\n",
    "        self.embedded_dim = embedded_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.is_pidirectional = is_pidirectional\n",
    "        if self.is_pidirectional:\n",
    "            self.lstm_unit_multipler = 2\n",
    "        else:\n",
    "             self.lstm_unit_multipler = 1\n",
    "        self.pad_idx = pad_idx\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        self.embedded = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedded_dim,\n",
    "                                     padding_idx=self.pad_idx)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedded_dim, hidden_size=self.lstm_units, \n",
    "                            batch_first=True, bidirectional=self.is_pidirectional)\n",
    "         \n",
    "        self.fc = nn.Linear(in_features=self.lstm_units * self.lstm_unit_multipler, out_features=self.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedded(x)\n",
    "        # the lstm output shape is Batch_size, seq_length, lstm_units it contains the hidden state of all timestamps\n",
    "        # hn is the final hidden state of shape lstm_layer_num, batch_size, lstm_uints\n",
    "        # cn is the final cell state of shape lstm_layer_num, batch_size, lstm_uints\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        x = self.fc(output)\n",
    "        # we will use cross entropy loss that will internally apply softmax\n",
    "        # x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def fit(self, train_dataloader, loss_fn, optimizer, epochs, device, callbacks=None):\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            acc_loss = 0\n",
    "\n",
    "            for train_data, train_gt in train_dataloader:\n",
    "\n",
    "                train_data = train_data.to(device)\n",
    "                train_gt = train_gt.to(device)\n",
    "\n",
    "                # training step\n",
    "                self.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = self.forward(train_data)\n",
    "\n",
    "                pred = pred.permute(0, 2, 1)\n",
    "                \n",
    "                loss = loss_fn(pred, train_gt)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                acc_loss += loss.item()\n",
    "            \n",
    "            acc_loss /= len(train_dataset)\n",
    "\n",
    "            print(f\"epoch {epoch + 1} / {epochs}: loss = {acc_loss}\")\n",
    "\n",
    "            self.writer.add_scalar(\"training_loss\", acc_loss, global_step=epoch)\n",
    "\n",
    "            # run call back functions\n",
    "            if callbacks is not None:\n",
    "                logs = {\"model\":self,\n",
    "                        \"device\":self.device,\n",
    "                        \"model_state_dict\": self.state_dict(),\n",
    "                        \"loss\": acc_loss\n",
    "                }\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lstm_model = Lstm(vocab_size=tokenizer.get_vocab_size(),\n",
    "                  embedded_dim=EMBEDDING_DIM,\n",
    "                  lstm_units=N_UNITS, pad_idx=pad_idx,\n",
    "                  is_pidirectional=False, log_dir=log_dir).to(device)\n",
    "\n",
    "print(lstm_model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_itr = iter(train_data_loader)\n",
    "sample_input, sample_output = next(loader_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lstm_model, input_size=(1, 4), dtypes=[sample_input.dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }  \n",
    "\n",
    "    def sample_from(self, probs, temperature):  \n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / torch.sum(probs)\n",
    "        sample_token = torch.multinomial(probs, 1).item()\n",
    "        return sample_token, probs\n",
    "\n",
    "    def generate(self, model, start_prompt, max_tokens, temperature, device):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ] \n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0: \n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.tensor([start_tokens]).to(device)\n",
    "                y = model(x).detach().to(\"cpu\") \n",
    "                # since we output logits with no softmax we will \n",
    "                # apply softmax here to get the probabilities\n",
    "                y_prob = torch.softmax(y, dim=-1)\n",
    "                sample_token, probs = self.sample_from(y_prob[0][-1], temperature)  \n",
    "                info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
    "                start_tokens.append(sample_token)  \n",
    "                start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if logs:\n",
    "            model = logs[\"model\"]\n",
    "            device = logs[\"device\"]\n",
    "            self.generate(model, \"recipe for\", max_tokens=100, temperature=1.0, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the LSTM <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [SaveCheckpoint(save_dir=checkpoint_dir, save_every=2),\n",
    "             TextGenerator(index_to_word=vocab_idnx_to_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_10.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    lstm_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(train_data_loader, loss_fn=loss_fn, optimizer=optimizer, \n",
    "               epochs=EPOCHS, device=device, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate text using the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(index_to_word=vocab_idnx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(info, vocab, top_k=5):\n",
    "    for i in info:\n",
    "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
    "        word_probs = i[\"word_probs\"]\n",
    "        p_sorted, i_sorted = torch.sort(word_probs, descending=True)\n",
    "        p_sorted = p_sorted[:top_k].numpy()\n",
    "        i_sorted = i_sorted[:top_k].numpy()\n",
    "        for p, i in zip(p_sorted, i_sorted):\n",
    "            round_prob = np.round(100*p,2)\n",
    "            print(f\"{vocab[i]}:   \\t{round_prob}%\")\n",
    "        print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(lstm_model,\n",
    "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=1.0, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = info[0][\"word_probs\"]\n",
    "top_k = 5\n",
    "p_sorted, i_sorted = torch.sort(word_probs, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_sorted.shape)\n",
    "print(p_sorted[:top_k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_probs(info, vocab_idnx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(lstm_model,\n",
    "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=0.2, device=device\n",
    ")\n",
    "print_probs(info, vocab_idnx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(lstm_model,\n",
    "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=1.0, device=device\n",
    ")\n",
    "print_probs(info, vocab_idnx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(lstm_model,\n",
    "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=0.2, device=device\n",
    ")\n",
    "print_probs(info, vocab_idnx_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
