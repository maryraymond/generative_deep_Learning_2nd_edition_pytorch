{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb956d5d",
   "metadata": {},
   "source": [
    "# ⚡️ Energy-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13610b0",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/07_ebm/01_ebm/ebm.ipynb) for Energy-Based models, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf907d5",
   "metadata": {},
   "source": [
    "_The original code is available [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition) and is licensed under the Apache License 2.0._\n",
    "_This implementation is distributed under the Apache License 2.0. See the LICENSE file for details._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28314a",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own Energy Based Model to predict the distribution of a demo dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519dd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/07_ebm/01_ebm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import random \n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e976d",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea251b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "# STEP_SIZE = 10\n",
    "STEP_SIZE = 10\n",
    "STEPS = 60\n",
    "NOISE = 0.005\n",
    "ALPHA = 0.1\n",
    "# ALPHA = 0.3\n",
    "GRADIENT_CLIP = 0.03\n",
    "# BATCH_SIZE = 128\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 8192\n",
    "LEARNING_RATE = 0.0001\n",
    "#LEARNING_RATE = 0.00001\n",
    "EPOCHS = 60\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb33ad",
   "metadata": {},
   "source": [
    "## 1. Preparing the data <a name=\"preparing the data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea58d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,)),\n",
    "     transforms.Pad((2,2), fill=-1)]\n",
    ")\n",
    "x_train = datasets.MNIST(root=data_dir, train=True,\n",
    "                        download=True, transform=transform)\n",
    "x_test = datasets.MNIST(root=data_dir, train=False,\n",
    "                        download=True, transform=transform)\n",
    "\n",
    "train_data_loader = DataLoader(x_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_data_loader = DataLoader(x_test, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95593b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x_train: {len(x_train)}\")\n",
    "print(f\"x_test: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e136f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_data_loader)\n",
    "sample, _ = next(data_iter)\n",
    "print(sample.shape)\n",
    "print(f\"min: {torch.min(sample)}, max: {torch.max(sample)}\")\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c0a77",
   "metadata": {},
   "source": [
    "## 2. Build the EBM network <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28482e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbnModel(nn.Module):\n",
    "    def __init__(self, input_channels, image_shape):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        self.conv2d_1 = nn.Conv2d(self.input_channels, \n",
    "                                  out_channels=16, \n",
    "                                  kernel_size=5, \n",
    "                                  stride=2, \n",
    "                                  padding=self._get_padding_size(self.image_shape[0], stride=2, kernal_size=5))\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=16, \n",
    "                                  out_channels=32, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=2, \n",
    "                                  padding=self._get_padding_size(self.image_shape[0] / 2, stride=2, kernal_size=3))\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=32, \n",
    "                                  out_channels=64, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=2, \n",
    "                                  padding=self._get_padding_size(self.image_shape[0] / (2*2), stride=2, kernal_size=3))\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels=64, \n",
    "                                  out_channels=64, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=2, \n",
    "                                  padding=self._get_padding_size(self.image_shape[0] / (2*2*2), stride=2, kernal_size=3))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, self.input_channels, *self.image_shape))\n",
    "            x = self.conv2d_1(dummy_input)\n",
    "            x = self.conv2d_2(x)\n",
    "            x = self.conv2d_3(x)\n",
    "            x = self.conv2d_4(x)\n",
    "            fc_input_size = x.view(1, -1).shape[1]\n",
    "        \n",
    "        self.fc_1 = nn.Linear(in_features=fc_input_size, out_features=64)\n",
    "        self.fc_2 =nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        self.activation = Swish()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc_2(x)\n",
    "        # x = F.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6018f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebn_instance = EbnModel(CHANNELS, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "summary(ebn_instance, (1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83239a5e",
   "metadata": {},
   "source": [
    "## 3. Set up a Langevin sampler function <a name=\"sampler\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604905df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, inp_imgs, steps, step_size, noise, device, return_img_per_step=False):\n",
    "    imgs_per_step = []\n",
    "    \n",
    "    # disable the gradients for the model parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inp_imgs = inp_imgs.detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # randn will produce random number with normal distrbuition of mean =0 and std=1 so to \n",
    "        # get std = noise we should multipley by noise\n",
    "        additional_noise = (torch.randn_like(inp_imgs) * noise)\n",
    "        inp_imgs = inp_imgs + additional_noise\n",
    "        inp_imgs = inp_imgs.clamp(min=-1.0, max=1.0)\n",
    "        inp_imgs.retain_grad()\n",
    "\n",
    "        if inp_imgs.grad is not None:\n",
    "            inp_imgs.grad.zero_()\n",
    "        \n",
    "        out_score = model(inp_imgs.to(device))\n",
    "        # calculate the gradient of the score with respect to the input images\n",
    "        out_score.sum().backward()\n",
    "        grad = inp_imgs.grad\n",
    "        grad = grad.clamp(min=-GRADIENT_CLIP, max=GRADIENT_CLIP)\n",
    "\n",
    "\n",
    "        # Gradient assent of the images\n",
    "        inp_imgs = inp_imgs + (step_size * grad)\n",
    "        inp_imgs = inp_imgs.clamp(min=-1.0, max=1.0)\n",
    "        inp_imgs = inp_imgs.detach().requires_grad_(True)\n",
    "        # output_imgs = inp_imgs.clone().detach()\n",
    "\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs)\n",
    "\n",
    "    # enable the gradients for the model parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    if return_img_per_step:\n",
    "        return_value = imgs_per_step\n",
    "    else:\n",
    "        return_value = inp_imgs\n",
    "    \n",
    "    return return_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a199b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    def __init__(self, model, batch_size, buffer_size, channels, image_size):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.examples =  [(torch.rand((1, self.channels, self.image_size, self.image_size)) * 2 ) - 1\n",
    "                          for _ in range(self.batch_size)]\n",
    "\n",
    "    \n",
    "    def sample_new_examples(self, steps, step_size, noise, device):\n",
    "        num_new_samples = np.random.binomial(self.batch_size, p=0.05)\n",
    "        rand_images = [(torch.rand((1, self.channels, self.image_size, self.image_size)) * 2 ) - 1\n",
    "                          for _ in range(num_new_samples)]\n",
    "        old_images = random.choices(self.examples, k=(self.batch_size-num_new_samples))\n",
    "        input_images = torch.cat(rand_images + old_images, dim=0)\n",
    "\n",
    "        input_images = generate_samples(self.model, input_images, \n",
    "                                        steps=steps, step_size=step_size, \n",
    "                                        noise=noise, device=device)\n",
    "\n",
    "        # add the images to the buffer and trancate it if it exceeded the buffer size\n",
    "        self.examples = list(torch.chunk(input_images, self.batch_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples [:self.buffer_size]\n",
    "\n",
    "        return input_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the buffer class\n",
    "buffer_test = Buffer(ebn_instance, BATCH_SIZE, BUFFER_SIZE, channels=CHANNELS, image_size=IMAGE_SIZE)\n",
    "sample = buffer_test.sample_new_examples(10, 10, NOISE, device)\n",
    "print(sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065265d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM (nn.Module):\n",
    "    def __init__(self, channels, image_size, batch_size, buffer_size, log_dir=\"./log\"):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_log_writter = SummaryWriter(log_dir=log_dir+\"/train/\")\n",
    "        self.test_log_writter = SummaryWriter(log_dir=log_dir+\"/val/\")\n",
    "        self.model = EbnModel(self.channels, (self.image_size, self.image_size))\n",
    "        self.buffer = Buffer(self.model, self.batch_size, self.buffer_size, \n",
    "                             self.channels, self.image_size)\n",
    "    \n",
    "    def fit(self, optimizer, train_data_loader, test_data_loader, epochs, device, \n",
    "            steps, step_size, callbacks=None, reg_alpha=0.1, noise=0.005):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.noise = noise\n",
    "        self.steps = steps\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            acc_train_loss = {\"cdiv_loss\":0, \"reg_loss\":0}\n",
    "            acc_test_loss = {\"cdiv_loss\":0, \"reg_loss\":0}\n",
    "\n",
    "            for train_data, _ in train_data_loader:\n",
    "                train_loss = self.train_step(train_data)\n",
    "\n",
    "                for key in train_loss.keys():\n",
    "                    acc_train_loss[key] += train_loss[key]\n",
    "            \n",
    "            for key in acc_train_loss.keys():\n",
    "                acc_train_loss[key] /= len(train_data_loader)\n",
    "\n",
    "            self.train_log_writter.add_scalar(\"cdiv_loss\", acc_train_loss[\"cdiv_loss\"], global_step=epoch)\n",
    "            self.train_log_writter.add_scalar(\"reg_loss\", acc_train_loss[\"reg_loss\"], global_step=epoch)\n",
    "            self.train_log_writter.add_scalar(\"total_loss\", acc_train_loss[\"cdiv_loss\"] + acc_train_loss[\"reg_loss\"], \n",
    "                                              global_step=epoch)\n",
    "\n",
    "            # running validation\n",
    "            for test_data, _ in test_data_loader:\n",
    "                test_loss = self.test_step(test_data)\n",
    "\n",
    "                for key in test_loss.keys():\n",
    "                    acc_test_loss[key] += test_loss[key]\n",
    "            \n",
    "            for key in acc_test_loss.keys():\n",
    "                acc_test_loss[key] /= len(test_data_loader)\n",
    "\n",
    "            self.test_log_writter.add_scalar(\"cdiv_loss\", acc_test_loss[\"cdiv_loss\"], global_step=epoch)\n",
    "            self.test_log_writter.add_scalar(\"reg_loss\", acc_test_loss[\"reg_loss\"], global_step=epoch)\n",
    "            self.test_log_writter.add_scalar(\"total_loss\", acc_test_loss[\"reg_loss\"] + acc_test_loss[\"cdiv_loss\"], \n",
    "                                             global_step=epoch)\n",
    "\n",
    "            print(f\"epoch {epoch}/{epochs}: train_cdiv_loss:{acc_train_loss['cdiv_loss']}, \"\n",
    "                  f\"train_reg_loss:{acc_train_loss['reg_loss']}, val_cdiv_loss:{acc_test_loss['cdiv_loss']}\"\n",
    "                  f\"val_reg_loss:{acc_test_loss['reg_loss']}\")\n",
    "            \n",
    "            if callbacks is not None:\n",
    "                logs = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\": self.model.state_dict(),\n",
    "                        \"loss\": acc_train_loss,\n",
    "                        \"examples\": torch.cat(random.choices(self.buffer.examples, k=10), dim=0),\n",
    "                        \"model\": self.model,\n",
    "                        \"device\": device}\n",
    "                \n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch=epoch, logs=logs)\n",
    "\n",
    "    @staticmethod\n",
    "    def contrastive_divergence_loss(real_images_scores, fake_images_scores):\n",
    "        return (torch.mean(fake_images_scores) - torch.mean(real_images_scores))\n",
    "    \n",
    "    def regularization_loss(self, real_images_scores, fake_images_scores):\n",
    "        return (self.reg_alpha * torch.mean(real_images_scores**2 + fake_images_scores**2))\n",
    "    \n",
    "    def train_step(self, train_real_images):\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # add noise to the real images\n",
    "        train_real_images = train_real_images.add(torch.randn_like(train_real_images) * self.noise)\n",
    "        train_real_images = train_real_images.clamp(min=-1.0, max=1.0)\n",
    "        train_real_images = train_real_images.to(self.device)\n",
    "\n",
    "        # get the fake images\n",
    "        fake_images = self.buffer.sample_new_examples(self.steps, self.step_size, self.noise, self.device).to(device)\n",
    "        input_images = torch.cat((train_real_images, fake_images), dim=0)\n",
    "\n",
    "        \n",
    "        scores = self.model(input_images)\n",
    "        real_scores, fake_scores = torch.chunk(scores, 2, dim=0)\n",
    " \n",
    "        cdiv_loss = self.contrastive_divergence_loss(real_scores, fake_scores)\n",
    "\n",
    "        reg_loss = self.regularization_loss(real_scores, fake_scores)\n",
    "\n",
    "        loss = cdiv_loss + reg_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        train_loss = {\"cdiv_loss\": cdiv_loss.item(), \"reg_loss\": reg_loss.item()}\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "    def test_step(self, test_real_images):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        fake_images = (torch.randn_like(test_real_images) * self.noise).to(self.device)\n",
    "        test_real_images = test_real_images.to(self.device)\n",
    "\n",
    "        input_images = torch.cat((test_real_images, fake_images), dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(input_images)\n",
    "            real_scores, fake_scores = torch.chunk(scores, 2)\n",
    "\n",
    "            cdiv_loss = self.contrastive_divergence_loss(real_scores, fake_scores)\n",
    "\n",
    "            reg_loss = self.regularization_loss(real_scores, fake_scores)\n",
    "\n",
    "            test_loss = {\"cdiv_loss\": cdiv_loss.item(), \"reg_loss\": reg_loss.item()}\n",
    "        \n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b789f1",
   "metadata": {},
   "source": [
    "Create the required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImages(Callback):\n",
    "    def __init__(self, num_images, noise, step_size, steps, save_dir=\"./gen_examples\"):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.save_dir = save_dir\n",
    "        self.noise = noise\n",
    "        self.steps = steps\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        device = logs[\"device\"]\n",
    "        model = logs[\"model\"]\n",
    "        examples = logs[\"examples\"]\n",
    "        epoch = logs[\"epoch\"]\n",
    "\n",
    "        intial_images = ((torch.rand(examples.shape) * 2) - 1)\n",
    "\n",
    "        generated_images = generate_samples(model, intial_images,\n",
    "                                            steps=self.steps, step_size=self.step_size,\n",
    "                                            noise=self.noise, device=device)\n",
    "        \n",
    "\n",
    "        display(generated_images, self.num_images, save_to=self.save_dir+f\"/generated_img_epoch_{epoch}.png\")\n",
    "\n",
    "        display(examples, self.num_images, save_to=self.save_dir+f\"/example_img_epoch_{epoch}.png\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c96f3",
   "metadata": {},
   "source": [
    "Prepare for EBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45711898",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [GenerateImages(10, noise=NOISE, step_size=1000, steps=STEPS, save_dir=sample_dir),\n",
    "             SaveCheckpoint(save_dir=checkpoint_dir, save_every=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = EBM(channels=CHANNELS, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n",
    "          buffer_size=BUFFER_SIZE, log_dir=log_dir).to(device)\n",
    "optimizer = Adam(ebm.model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5954417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_270.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    ebm.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm.fit(optimizer=optimizer, train_data_loader=train_data_loader, test_data_loader=test_data_loader,\n",
    "        epochs=EPOCHS, device=device, steps=STEPS, step_size=STEP_SIZE, callbacks=callbacks,\n",
    "        reg_alpha=ALPHA, noise=NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10878b72",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_images = ((torch.rand((10, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)) * 2) - 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(start_images, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891661db",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generate_samples(ebm.model, start_images, steps=1000, step_size=STEP_SIZE, \n",
    "                                    device=device, noise=NOISE, return_img_per_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(generated_images))\n",
    "print(generated_images[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(generated_images[-1], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i in [0, 1, 3, 5, 10, 30, 50, 100, 300, 999]:\n",
    "    imgs.append(generated_images[i][6])\n",
    "\n",
    "display(torch.stack(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_data_loader)\n",
    "sample, _ = next(data_iter)\n",
    "print(ebm.model(sample.to(device))[0].item())\n",
    "print(ebm.model(start_images.to(device))[0].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
