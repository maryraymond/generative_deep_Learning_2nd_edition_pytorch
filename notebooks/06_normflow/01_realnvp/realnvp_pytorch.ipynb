{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ€ RealNVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/06_normflow/01_realnvp/realnvp.ipynb) for normalizing flow model, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The original code is available [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition) and is licensed under the Apache License 2.0._\n",
    "_This implementation is distributed under the Apache License 2.0. See the LICENSE file for details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own RealNVP network to predict the distribution of a demo dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/06_normflow/01_realnvp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUPLING_DIM = 256\n",
    "COUPLING_LAYERS = 2\n",
    "INPUT_DIM = 2\n",
    "REGULARIZATION = 0.01\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 300\n",
    "\n",
    "NUM_SAMPLES = 30000\n",
    "# If we used the same learning rate as Keras 0.0001 the training is unstable\n",
    "# and eventualy the loss starts increasing towards the end of the 300 epochs\n",
    "# so we are setting the learning rate to 0.00001 since it is much stable here\n",
    "LEARNING_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take the point distrbuiton and ignor the labels\n",
    "data = datasets.make_moons(NUM_SAMPLES, noise=0.05)[0].astype(\"float32\")\n",
    "\n",
    "# we wil normaliza the data\n",
    "mean_x, mean_y = np.mean(data[:, 0]), np.mean(data[:, 1])\n",
    "std_x, std_y = np.std(data[:, 0]), np.std(data[:, 1])\n",
    "print(f\"orginal data mean = ({mean_x}, {mean_y}), orginal data var = ({std_x}, {std_y})\")\n",
    "\n",
    "normalized_data = data.copy()\n",
    "normalized_data[:, 0] = (normalized_data[:, 0] - mean_x) / std_x \n",
    "normalized_data[:, 1] = (normalized_data[:, 1] - mean_y) / std_y \n",
    "\n",
    "norm_mean_x, norm_mean_y = np.mean(normalized_data[:, 0]), np.mean(normalized_data[:, 1])\n",
    "norm_var_x, norm_var_y = np.var(normalized_data[:, 0]), np.var(normalized_data[:, 1])\n",
    "\n",
    "print(f\"normalized mean = ({norm_mean_x}, {norm_mean_x}), normalized var = ({norm_var_x}, {norm_var_y})\")\n",
    "\n",
    "# visualize the data\n",
    "plt.scatter(normalized_data[:, 0], normalized_data[:, 1], c=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datatset \n",
    "train_dataset = TensorDataset(torch.from_numpy(normalized_data))\n",
    "# create a data loader\n",
    "train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the RealNVP network <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coupling(Module):\n",
    "    def __init__(self, input_dim, coupling_dim, reg,  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.coupling_dim = coupling_dim\n",
    "        self.reg = reg\n",
    "\n",
    "        # defining the coupling internal layers\n",
    "        \n",
    "        # scale layers\n",
    "        self.s_layer_1 = nn.Linear(self.input_dim, self.coupling_dim)\n",
    "        self.s_layer_2 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.s_layer_3 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.s_layer_4 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.s_layer_5 = nn.Linear(self.coupling_dim, self.input_dim)\n",
    "\n",
    "        # translation layers\n",
    "        self.t_layer_1 = nn.Linear(self.input_dim, self.coupling_dim)\n",
    "        self.t_layer_2 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.t_layer_3 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.t_layer_4 = nn.Linear(self.coupling_dim, self.coupling_dim)\n",
    "        self.t_layer_5 = nn.Linear(self.coupling_dim, self.input_dim)\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_loss = 0.0\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                l2_loss += torch.sum(layer.weight **2)\n",
    "        \n",
    "        return self.reg * l2_loss\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.s_layer_1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.s_layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.s_layer_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.s_layer_4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.s_layer_5(x)\n",
    "        s = F.tanh(x)\n",
    "\n",
    "        x = self.t_layer_1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.t_layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.t_layer_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.t_layer_4(x)\n",
    "        x = F.relu(x)\n",
    "        t = self.t_layer_5(x)\n",
    "\n",
    "        return [s, t]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_coupling = Coupling(input_dim=INPUT_DIM, coupling_dim=COUPLING_DIM, reg=REGULARIZATION)\n",
    "summary(sample_coupling, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = torch.tensor([0.0, 0.0])\n",
    "scale_diag = torch.tensor([1.0, 1.0])\n",
    "covariance_matrix = torch.diag(scale_diag)\n",
    "print(covariance_matrix)\n",
    "gaussian_distribution = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=covariance_matrix)\n",
    "\n",
    "gaussian_samples = [gaussian_distribution.sample() for i in range(3000)]\n",
    "print(len(gaussian_samples))\n",
    "\n",
    "plt.scatter([point[0] for point in gaussian_samples], [point[1] for point in gaussian_samples], c=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the RealNVP network <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(Module):\n",
    "    def __init__(self, input_dim, num_coupling_layers, coupling_dim, reg, device=\"cpu\", log_dir=\"./\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "        self.coupling_dim = coupling_dim\n",
    "        self.reg = reg\n",
    "        self.writter = SummaryWriter(log_dir)\n",
    "        self.device = device\n",
    "\n",
    "        covariance_matrix = torch.diag(torch.tensor([1.0, 1.0])).to(self.device)\n",
    "        self.gaussian_distribution = torch.distributions.MultivariateNormal(loc=torch.as_tensor([0.0, 0.0]).to(device), \n",
    "                                                                            covariance_matrix=covariance_matrix)\n",
    "        self.masks = torch.tensor([[0, 1], [1, 0]] * (self.num_coupling_layers // 2), dtype=torch.float32).to(device)\n",
    "        self.coupling_layers = nn.ModuleList()\n",
    "        for i in range(self.num_coupling_layers):\n",
    "            self.coupling_layers.append(Coupling(self.input_dim, self.coupling_dim, reg=self.reg))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        forward_path = 1\n",
    "        backward_path = -1\n",
    "\n",
    "        log_det_inv = 0\n",
    "\n",
    "        # This is contraditing the orginal code in Keras where the direction is set to 1 \n",
    "        # during the inference  and -1 one during training, this does not seem right\n",
    "        # since for the training we should use the function z = f(x) and while predicting\n",
    "        # we should be using the function x = g(z) according to the text of the book,\n",
    "        # and for the funcion used x =  inverse_mask[i] * (x * torch.exp(direction*s) + (direction * t) * (torch.exp(gate * s))) + x_masked\n",
    "        # to evaluate correctly this mean the training direction should be 1 and the evaluation direction should be -1\n",
    "\n",
    "        direction = backward_path\n",
    "\n",
    "        if self.training:\n",
    "            direction = forward_path\n",
    "\n",
    "        # inverse_mask = 1 - self.masks\n",
    "\n",
    "        # will be zero if not training\n",
    "        gate = (direction - 1) / 2\n",
    "        loss_gate = (direction + 1 ) / -2\n",
    "\n",
    "        for i in range(self.num_coupling_layers)[::-1*direction]:\n",
    "            \n",
    "            s, t = self.coupling_layers[i](x*self.masks[i])\n",
    "            inverse_mask = 1 - self.masks[i]\n",
    "            \n",
    "            # we will use the values for the other input only\n",
    "            # we can not modifiy in place i.e: s *= inverse_mask\n",
    "            s = s * inverse_mask\n",
    "            t = t * inverse_mask\n",
    "\n",
    "            x_masked = x * self.masks[i]\n",
    "            x =  inverse_mask * (x * torch.exp(direction*s) + (direction * t) * (torch.exp(gate * s))) + x_masked\n",
    "\n",
    "            log_det_inv = log_det_inv + (loss_gate * torch.sum(s, dim=1)) \n",
    "\n",
    "        return x, log_det_inv\n",
    "    \n",
    "    def log_loss(self, x):\n",
    "        z, log_det = self(x)\n",
    "        neg_log_x = (-1*self.gaussian_distribution.log_prob(z)) + log_det\n",
    "        loss = neg_log_x.mean()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, training_data_loader, optimizer, epochs, eval_data_loader=None, callbacks=None):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            acc_training_loss = 0\n",
    "            acc_eval_loss = 0\n",
    "\n",
    "            for training_data, in training_data_loader:\n",
    "            \n",
    "                train_loss = self.train_step(training_data.to(self.device))\n",
    "                acc_training_loss += train_loss\n",
    "            \n",
    "            acc_training_loss /= len(training_data_loader)\n",
    "            self.writter.add_scalar(\"train_loss\", acc_training_loss, global_step = epoch)\n",
    "\n",
    "            # check if we have a validation dataset\n",
    "            if eval_data_loader:\n",
    "\n",
    "                for eval_data, in eval_data_loader:\n",
    "                    eval_loss = self.test_step(eval_data)\n",
    "                    acc_eval_loss +=  eval_loss\n",
    "                \n",
    "                acc_eval_loss /= len(eval_data_loader)\n",
    "                self.writter.add_scalar(\"eval_loss\", acc_eval_loss, global_step = epoch)\n",
    "\n",
    "\n",
    "            print(f\"epoch {epoch} : {epochs} training_loss: {acc_training_loss}\")\n",
    "            \n",
    "            if callbacks:\n",
    "                logs = {\"device\": self.device,\n",
    "                        \"gaussian_distribution\": self.gaussian_distribution,\n",
    "                        \"model\": self,\n",
    "                        \"loss\": train_loss}\n",
    "                \n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs)   \n",
    "\n",
    "    def train_step(self, train_data):\n",
    "\n",
    "        self.train()\n",
    "        # the network call will be done inside the loss function itself\n",
    "        loss = self.log_loss(train_data)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, eval_data):\n",
    "\n",
    "        # This looks weired but we want to forward fucntion to take the same \n",
    "        # path of z = f(x) since we are still using x data so to force this pat\n",
    "        # we will set the model to training\n",
    "        self.train()\n",
    "        # the network call will be done inside the loss function itself\n",
    "        loss = self.log_loss(eval_data)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratImage(Callback):\n",
    "    def __init__(self, num_samples, normalized_data, save_dir):\n",
    "        super().__init__()\n",
    "        self.number_samples = num_samples\n",
    "        self.save_dir = save_dir\n",
    "        self.normalized_data = normalized_data\n",
    "\n",
    "    def generate(self, model, gaussian_distribution, device):\n",
    "        # get z from x \n",
    "        model.train()\n",
    "        z_pred, _ = model(self.normalized_data.to(device))\n",
    "\n",
    "        # sample a gaussian distrbution\n",
    "        z = gaussian_distribution.sample((self.number_samples,))\n",
    "        # call the model to generate x\n",
    "        model.eval()\n",
    "        x_pred, _ = model(z.to(device))\n",
    "\n",
    "        return x_pred, z_pred,  z\n",
    "        \n",
    "    def display(self, x, z, samples, normalized_data, save_to=None):\n",
    "        f, axes = plt.subplots(2, 2)\n",
    "        f.set_size_inches(8, 5)\n",
    "\n",
    "        axes[0, 0].scatter(\n",
    "            normalized_data[:, 0], normalized_data[:, 1], color=\"r\", s=1\n",
    "        )\n",
    "        axes[0, 0].set(title=\"Data space X\", xlabel=\"x_1\", ylabel=\"x_2\")\n",
    "        axes[0, 0].set_xlim([-2, 2])\n",
    "        axes[0, 0].set_ylim([-2, 2])\n",
    "        axes[0, 1].scatter(z[:, 0], z[:, 1], color=\"r\", s=1)\n",
    "        axes[0, 1].set(title=\"f(X)\", xlabel=\"z_1\", ylabel=\"z_2\")\n",
    "        axes[0, 1].set_xlim([-2, 2])\n",
    "        axes[0, 1].set_ylim([-2, 2])\n",
    "        axes[1, 0].scatter(samples[:, 0], samples[:, 1], color=\"g\", s=1)\n",
    "        axes[1, 0].set(title=\"Latent space Z\", xlabel=\"z_1\", ylabel=\"z_2\")\n",
    "        axes[1, 0].set_xlim([-2, 2])\n",
    "        axes[1, 0].set_ylim([-2, 2])\n",
    "        axes[1, 1].scatter(x[:, 0], x[:, 1], color=\"g\", s=1)\n",
    "        axes[1, 1].set(title=\"g(Z)\", xlabel=\"x_1\", ylabel=\"x_2\")\n",
    "        axes[1, 1].set_xlim([-2, 2])\n",
    "        axes[1, 1].set_ylim([-2, 2])\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0.6)\n",
    "        if save_to:\n",
    "            plt.savefig(save_to)\n",
    "            print(f\"\\nSaved to {save_to}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model = logs[\"model\"]\n",
    "        device = logs[\"device\"]\n",
    "        gaussian_distribution = logs[\"gaussian_distribution\"]\n",
    "\n",
    "        x, z, samples = self.generate(model, gaussian_distribution, device)\n",
    "        \n",
    "        z = z.detach().to(\"cpu\").numpy()\n",
    "        x = x.detach().to(\"cpu\").numpy()\n",
    "        samples = samples.detach().to(\"cpu\").numpy()\n",
    "\n",
    "        normalized_data_numpy = self.normalized_data.detach().to(\"cpu\").numpy()\n",
    "\n",
    "        save_path = f\"{self.save_dir}/generated_image_{epoch}.png\"\n",
    "        self.display(x, z, samples, normalized_data_numpy, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model\"].state_dict(),\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [SaveCheckpoint(checkpoint_dir, 10),\n",
    "             GeneratImage(3000, torch.from_numpy(normalized_data), sample_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "realnvp = RealNVP(INPUT_DIM, COUPLING_LAYERS, COUPLING_DIM, REGULARIZATION, device, log_dir)\n",
    "summary(realnvp, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(realnvp.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realnvp.fit(train_data_loader, optimizer, EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache from unused memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = torch.diag(torch.tensor([1.0, 1.0])).to(device)\n",
    "gaussian_distribution = torch.distributions.MultivariateNormal(loc=torch.as_tensor([0.0, 0.0]).to(device), \n",
    "                                                               covariance_matrix=covariance_matrix)\n",
    "\n",
    "test_gen_images = GeneratImage(3000, \n",
    "                               torch.from_numpy(normalized_data), \n",
    "                               save_dir=None)\n",
    "\n",
    "realnvp.eval()\n",
    "x, z, samples = test_gen_images.generate(realnvp, gaussian_distribution, device)\n",
    "\n",
    "x = x.detach().to(\"cpu\").numpy()\n",
    "z = z.detach().to(\"cpu\").numpy()\n",
    "samples = samples.detach().to(\"cpu\").numpy()\n",
    "\n",
    "test_gen_images.display(x, z, samples, normalized_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
