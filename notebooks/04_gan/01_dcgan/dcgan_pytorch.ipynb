{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§± DCGAN - Bricks Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/04_gan/01_dcgan/dcgan.ipynb) for Deep Convolutional GAN, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The original code is available [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition) and is licensed under the Apache License 2.0._\n",
    "_This implementation is distributed under the Apache License 2.0. See the LICENSE file for details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own DCGAN on the bricks dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/04_gan/01_dcgan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from notebooks.utils import display\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "EPOCHS = 300\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.999\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/lego-brick-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "full_data = datasets.ImageFolder(dataset_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"full dataset size = \", len(full_data))\n",
    "print(\"full dataset labels = \", full_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to use the data in the dataset folder only\n",
    "required_class = \"dataset\"\n",
    "req_class_idx = full_data.class_to_idx[required_class]\n",
    "req_idxs = [i for i, (_, label) in enumerate(full_data) if label==req_class_idx]\n",
    "print(\"size of required indces = \", len(req_idxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Subset(full_data, req_idxs)\n",
    "print(\"size of training dataset = \", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data_loader)\n",
    "sample_images, _ = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the GAN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, image_size):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # layer group 1\n",
    "        p = self._get_padding_size(input_w=self.image_size, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.channels, out_channels=64, \n",
    "                                kernel_size=4, stride=2, padding=p, bias=False)\n",
    "        \n",
    "        # layer group 2 \n",
    "        p = self._get_padding_size(input_w=self.image_size/2, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, \n",
    "                               kernel_size=4, stride=2, padding=p, bias=False)\n",
    "        \n",
    "        self.bn_2 = nn.BatchNorm2d(num_features= 128, momentum=0.9)\n",
    "\n",
    "\n",
    "        # layer group 3\n",
    "        p = self._get_padding_size(input_w=self.image_size/4, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_3 = nn.Conv2d(in_channels=128, out_channels=256, \n",
    "                               kernel_size=4, stride=2, padding=p, bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(num_features=256, momentum=0.9)\n",
    "\n",
    "        # layer group 4\n",
    "        p = self._get_padding_size(input_w=self.image_size/8, stride=2, kernal_size=4)\n",
    "        self.conv_4 = nn.Conv2d(in_channels=256, out_channels=512, \n",
    "                               kernel_size=4, stride=2, padding=p, bias=False)\n",
    "        self.bn_4 = nn.BatchNorm2d(num_features=512, momentum=0.9)\n",
    "\n",
    "        self.conv_5 = nn.Conv2d(in_channels=512, out_channels=1, \n",
    "                               kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_5(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        x = x.view((B,1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator(CHANNELS, IMAGE_SIZE)\n",
    "\n",
    "print(discriminator.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(discriminator, (1,CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(in_channels=self.num_dim, out_channels=512, \n",
    "                                               kernel_size=4, stride=1, padding=0, bias=False, output_padding=0)\n",
    "        self.bn_1 = nn.BatchNorm2d(num_features=512, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4, stride=2, kernal_size=4)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        \n",
    "        self.bn_2 = nn.BatchNorm2d(num_features=256, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*2, stride=2, kernal_size=4)\n",
    "        self.conv_trans_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(num_features=128, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*4, stride=2, kernal_size=4) \n",
    "        self.conv_trans_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_4 = nn.BatchNorm2d(num_features=64, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*8, stride=2, kernal_size=4)\n",
    "        self.conv_trans_5 = nn.ConvTranspose2d(in_channels=64, out_channels=self.channels, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.floor(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.view((B, self.num_dim, 1, 1))\n",
    "        x = self.conv_trans_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_5(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(Z_DIM, CHANNELS)\n",
    "print(generator.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(generator, (1, Z_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN (nn.Module):\n",
    "    def __init__(self, num_dim, channels, image_size, log_dir=\"./log/\"):\n",
    "        super().__init__()\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.generator = Generator(num_dim, channels)\n",
    "        self.discriminator = Discriminator(channels, image_size)\n",
    "\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "    \n",
    "    # this function will not be used, we just implment it to be able to use\n",
    "    # the torchinfo summary function\n",
    "    def forward(self, x):\n",
    "        x = self.generator(x)\n",
    "        x = self.discriminator(x)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # set the dicremenator and generator to training mode\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        # zero the grads\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "        # generate fake images\n",
    "        B = real_images.shape[0]\n",
    "        input_noise = torch.randn((B, self.num_dim)).to(self.device)\n",
    "\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        fake_images = self.generator(input_noise)\n",
    "\n",
    "        # train discremeantor\n",
    "        fake_pred = self.discriminator(fake_images)\n",
    "        real_pred = self.discriminator(real_images)\n",
    "\n",
    "        # prepare labels\n",
    "        fake_lables = torch.zeros_like(fake_pred) + (self.label_noise * torch.rand_like(fake_pred))\n",
    "        # fake_lables = torch.clamp(fake_lables, min=0.0)\n",
    "        real_lables = torch.ones_like(real_pred) - (self.label_noise * torch.rand_like(real_pred))\n",
    "        # real_lables = torch.clamp(real_lables, max=1.0)\n",
    "\n",
    "        # fake_lables = torch.zeros_like(fake_pred) \n",
    "        # real_lables = torch.ones_like(real_pred) \n",
    "\n",
    "        #calculate dicremenator loss \n",
    "        d_loss_fake = self.d_loss_fn(fake_pred, fake_lables)\n",
    "        d_loss_real = self.d_loss_fn(real_pred, real_lables)\n",
    "        d_loss = (d_loss_fake + d_loss_real) / 2\n",
    "\n",
    "        # calculate gradiants\n",
    "        d_loss.backward()\n",
    "        # update waits\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        # train generator\n",
    "        fake_images = self.generator(input_noise)\n",
    "        # train discremeantor\n",
    "        fake_pred = self.discriminator(fake_images)\n",
    "\n",
    "        g_loss = self.g_loss_fn(fake_pred, real_lables)\n",
    "        #cal gradients\n",
    "        g_loss.backward()\n",
    "        #update waits\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        loss_dict = {\"d_loss_fake\":d_loss_fake.item(), \"d_loss_real\":d_loss_real.item(), \n",
    "                     \"d_loss\": d_loss.item(), \"g_loss\":g_loss.item()}\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    def fit(self, training_dataloader, epochs, g_optimizer, d_optimizer, \n",
    "            d_loss_fn, g_loss_fn, device, labels_noise=0.1, callbacks=None):\n",
    "        \n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.label_noise = labels_noise\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "\n",
    "            losses = {\"d_fake_loss_acc\": 0,\n",
    "                      \"d_real_loss_acc\": 0,\n",
    "                      \"d_loss_acc\": 0,\n",
    "                      \"g_loss_acc\": 0}\n",
    "\n",
    "            # loop over all data in the training set\n",
    "            for images, _ in training_dataloader:\n",
    "\n",
    "                # run training_step\n",
    "                loss_dict = self.train_step(images)\n",
    "                losses[\"d_fake_loss_acc\"] += loss_dict[\"d_loss_fake\"]\n",
    "                losses[\"d_real_loss_acc\"] += loss_dict[\"d_loss_real\"]\n",
    "                losses[\"d_loss_acc\"] += loss_dict[\"d_loss\"]\n",
    "                losses[\"g_loss_acc\"] += loss_dict[\"g_loss\"]\n",
    "            \n",
    "\n",
    "            losses[\"d_fake_loss_acc\"] /= len(train_data)\n",
    "            losses[\"d_real_loss_acc\"] /= len(train_data)\n",
    "            losses[\"d_loss_acc\"] /= len(train_data)\n",
    "            losses[\"g_loss_acc\"] /= len(train_data)\n",
    "            \n",
    "            # print epoch progress\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: d_fake_loss: {losses['d_fake_loss_acc'] :.4f} \"\n",
    "                f\" d_real_loss: {losses['d_real_loss_acc']:.4f} \"\n",
    "                f\" d_loss: {losses['d_loss_acc']:.4f}\"\n",
    "                f\" g_loss: {losses['g_loss_acc']:.4f}\"\n",
    "            )\n",
    "            # log loss to tensorboard\n",
    "            self.writer_train.add_scalar(\"d_fake_loss\", losses[\"d_fake_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"d_real_loss\", losses[\"d_real_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"d_loss\",losses[\"d_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"g_loss\", losses[\"g_loss_acc\"], global_step=i)\n",
    "            \n",
    "            # run call back functions\n",
    "            if callbacks is not None:\n",
    "                logs = {\"device\":self.device,\n",
    "                        \"generator\":self.generator,\n",
    "                        \"model_state_dict\": self.state_dict(),\n",
    "                        \"loss\": losses\n",
    "                }\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(i, logs=logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImages(Callback):\n",
    "    def __init__(self, num_images, latent_dim, save_dir=\"./gen_examples\"):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.latent_dim = latent_dim\n",
    "        self.save_dir = save_dir\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        device = logs[\"device\"]\n",
    "        generator = logs[\"generator\"]\n",
    "\n",
    "        input_noise = torch.randn((self.num_images, self.latent_dim)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generator.eval()\n",
    "            # scale back to 0 to 255\n",
    "            gen_imgs = generator(input_noise).detach() * 127.5 + 127.5\n",
    "            display(gen_imgs, save_to=self.save_dir+f\"/epoch_{epoch}.png\")\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DCGAN object and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan = DCGAN(Z_DIM, CHANNELS, IMAGE_SIZE, log_dir).to(device)\n",
    "print(dcgan.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(dcgan, (1, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss_function = nn.BCELoss()\n",
    "d_loss_function = nn.BCELoss()\n",
    "\n",
    "g_optimizer = optim.Adam(dcgan.generator.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])\n",
    "d_optimizer = optim.Adam(dcgan.discriminator.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [GenerateImages(10, Z_DIM, save_dir=sample_dir),\n",
    "             SaveCheckpoint(save_dir=checkpoint_dir, save_every=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_270.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    dcgan.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan.fit(train_data_loader, epochs=EPOCHS, g_optimizer=g_optimizer, d_optimizer=d_optimizer,\n",
    "          d_loss_fn=d_loss_function, g_loss_fn=g_loss_function, device=device, callbacks=callbacks,\n",
    "          labels_noise=NOISE_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new images <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some points in the latent space, from the standard normal distribution\n",
    "grid_width, grid_height = (10, 3)\n",
    "z_sample = torch.randn((grid_width * grid_height, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dcgan.eval()\n",
    "    reconstructions = dcgan.generator(z_sample.to(device))\n",
    "    \n",
    "reconstructions_np = reconstructions.to(\"cpu\").permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions_np[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(img1, img2):\n",
    "    return torch.mean(torch.abs(img1 - img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for images, _ in train_data_loader:\n",
    "        all_data.extend(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 3, 5\n",
    "fig, axs = plt.subplots(r, c, figsize=(10, 6))\n",
    "fig.suptitle(\"Generated images\", fontsize=20)\n",
    "\n",
    "noise = torch.randn((r * c, Z_DIM))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dcgan.eval()\n",
    "    gen_imgs = dcgan.generator(noise.to(device))\n",
    "\n",
    "\n",
    "gen_imgs_np = gen_imgs.to(\"cpu\").permute(0, 2, 3, 1).numpy()\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(gen_imgs_np[cnt], cmap=\"gray_r\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(r, c, figsize=(10, 6))\n",
    "fig.suptitle(\"Closest images in the training set\", fontsize=20)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        c_diff = 99999\n",
    "        c_img = None\n",
    "        for k_idx, k in enumerate(all_data):\n",
    "            diff = compare_images(gen_imgs[cnt].to(\"cpu\"), k.to(\"cpu\"))\n",
    "            if diff < c_diff:\n",
    "                c_img = k.permute(1, 2, 0).numpy()\n",
    "                c_diff = diff\n",
    "        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
