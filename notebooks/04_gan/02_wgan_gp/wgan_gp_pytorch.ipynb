{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤ª WGAN - CelebA Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/mary/work/repos/generative_deep_Learning_2nd_edition_pytorch\"\n",
    "exp_dir = working_dir + \"/notebooks/04_gan/02_wgan_gp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the notebooks folder\n",
    "notebooks_path = os.path.abspath(working_dir)\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.append(notebooks_path)\n",
    "\n",
    "utils_path = os.path.abspath(exp_dir)\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from notebooks.utils import display\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 512\n",
    "Z_DIM = 128\n",
    "LEARNING_RATE = 0.0002\n",
    "EPOCHS = 200\n",
    "CRITIC_STEPS = 3\n",
    "GP_WEIGHT = 10.0\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/celeba-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "\n",
    "train_data, _ = torch.utils.data.random_split(train_data, [0.1, 0.9])\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"full dataset size = \", len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data_loader)\n",
    "sample_images, _ = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the GAN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, channels, image_size):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # layer group 1\n",
    "        p = self._get_padding_size(input_w=self.image_size, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.channels, out_channels=64, \n",
    "                                kernel_size=4, stride=2, padding=p)\n",
    "        \n",
    "        # layer group 2 \n",
    "        p = self._get_padding_size(input_w=self.image_size/2, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "        \n",
    "        # layer group 3\n",
    "        p = self._get_padding_size(input_w=self.image_size/4, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_3 = nn.Conv2d(in_channels=128, out_channels=256, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "\n",
    "        # layer group 4\n",
    "        p = self._get_padding_size(input_w=self.image_size/8, stride=2, kernal_size=4)\n",
    "        self.conv_4 = nn.Conv2d(in_channels=256, out_channels=512, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "\n",
    "        self.conv_5 = nn.Conv2d(in_channels=512, out_channels=1, \n",
    "                               kernel_size=4, stride=1, padding=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_5(x)\n",
    "\n",
    "        x = x.view((B,1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(CHANNELS, IMAGE_SIZE)\n",
    "\n",
    "print(critic.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(critic, (1,CHANNELS, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(in_channels=self.num_dim, out_channels=512, \n",
    "                                               kernel_size=4, stride=1, padding=0, bias=False, output_padding=0)\n",
    "        self.bn_1 = nn.BatchNorm2d(num_features=512, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4, stride=2, kernal_size=4)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        \n",
    "        self.bn_2 = nn.BatchNorm2d(num_features=256, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*2, stride=2, kernal_size=4)\n",
    "        self.conv_trans_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(num_features=128, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*4, stride=2, kernal_size=4) \n",
    "        self.conv_trans_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_4 = nn.BatchNorm2d(num_features=64, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*8, stride=2, kernal_size=4)\n",
    "        self.conv_trans_5 = nn.ConvTranspose2d(in_channels=64, out_channels=self.channels, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.floor(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.view((B, self.num_dim, 1, 1))\n",
    "        x = self.conv_trans_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_5(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(Z_DIM, CHANNELS)\n",
    "print(generator.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(generator, (1, Z_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_critic_loss(fake_pred, real_pred):\n",
    "    w_loss = torch.mean(fake_pred) - torch.mean(real_pred)\n",
    "    return w_loss\n",
    "\n",
    "def wasserstein_generator_loss(fake_pred):\n",
    "    w_loss = -1 * torch.mean(fake_pred)\n",
    "    return w_loss\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images):\n",
    "    B = real_images.shape[0]\n",
    "    alpha = torch.randn((B, 1, 1, 1)).to(real_images.device)\n",
    "    interpolated = (alpha * fake_images) + ((1 - alpha) * real_images)\n",
    "    interpolated.requires_grad_(True)\n",
    "    pred = critic(interpolated)\n",
    "\n",
    "    # calculate the gradient of the output with respect to the input\n",
    "    gradients = autograd.grad(outputs=pred, inputs=interpolated, grad_outputs=torch.ones_like(pred),\n",
    "                  create_graph=True, only_inputs=True)[0]\n",
    "    # flaten the gradients for each image\n",
    "    gradients = gradients.view(B, -1)\n",
    "    # L2 norm\n",
    "    # grad_norm = torch.sqrt(torch.sum(torch.square(gradients)))\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "    gp = torch.mean((grad_norm - 1)**2)\n",
    "\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN (nn.Module):\n",
    "    def __init__(self, num_dim, channels, image_size, log_dir=\"./log/\"):\n",
    "        super().__init__()\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.generator = Generator(num_dim, channels)\n",
    "        self.critic = Critic(channels, image_size)\n",
    "\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "    \n",
    "    # this function will not be used, we just implment it to be able to use\n",
    "    # the torchinfo summary function\n",
    "    def forward(self, x):\n",
    "        x = self.generator(x)\n",
    "        x = self.critic(x)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # set the dicremenator and generator to training mode\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "\n",
    "        # generate fake images\n",
    "        B = real_images.shape[0]\n",
    "\n",
    "        # We train the critic more times per step\n",
    "        for i in range(self.critic_steps):\n",
    "            # zero the grads\n",
    "            self.c_optimizer.zero_grad()\n",
    "            input_noise = torch.randn((B, self.num_dim)).to(self.device)\n",
    "\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            fake_images = self.generator(input_noise)\n",
    "\n",
    "            # train discremeantor\n",
    "            fake_pred = self.critic(fake_images)\n",
    "            real_pred = self.critic(real_images)\n",
    "\n",
    "            #calculate dicremenator loss \n",
    "            c_w_loss = self.c_w_loss_fn(fake_pred=fake_pred, real_pred=real_pred)\n",
    "            c_gp_loss = self.c_gp_loss_fn(critic=self.critic, real_images=real_images, fake_images=fake_images)\n",
    "            c_total_loss = c_w_loss + (self.gp_lambda * c_gp_loss)\n",
    "\n",
    "\n",
    "            # calculate gradiants\n",
    "            c_total_loss.backward()\n",
    "            # update waits\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        # zero the grads\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        fake_images = self.generator(input_noise)\n",
    "        # train discremeantor\n",
    "        fake_pred = self.critic(fake_images)\n",
    "\n",
    "        g_w_loss = self.g_w_loss_fn(fake_pred)\n",
    "        #cal gradients\n",
    "        g_w_loss.backward()\n",
    "        #update waits\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        loss_dict = {\"c_w_loss\":c_w_loss.item(), \"c_gp_loss\":c_gp_loss.item(), \n",
    "                     \"c_total_loss\": c_total_loss.item(), \"g_w_loss\":g_w_loss.item()}\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    def fit(self, training_dataloader, epochs, g_optimizer, c_optimizer, \n",
    "            c_w_loss_fn, c_gp_loss, g_w_loss_fn, gp_lambda, device, \n",
    "            critic_steps=3, callbacks=None):\n",
    "        \n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.c_w_loss_fn = c_w_loss_fn\n",
    "        self.g_w_loss_fn = g_w_loss_fn\n",
    "        self.c_gp_loss_fn = c_gp_loss\n",
    "        self.gp_lambda = gp_lambda\n",
    "        self.critic_steps = critic_steps\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "\n",
    "            losses = {\"c_w_loss_acc\": 0,\n",
    "                      \"c_gp_loss_acc\": 0,\n",
    "                      \"c_total_loss_acc\": 0,\n",
    "                      \"g_w_loss_acc\": 0}\n",
    "\n",
    "            # loop over all data in the training set\n",
    "            for images, _ in training_dataloader:\n",
    "\n",
    "                # run training_step\n",
    "                loss_dict = self.train_step(images)\n",
    "                losses[\"c_w_loss_acc\"] += loss_dict[\"c_w_loss\"]\n",
    "                losses[\"c_gp_loss_acc\"] += loss_dict[\"c_gp_loss\"]\n",
    "                losses[\"c_total_loss_acc\"] += loss_dict[\"c_total_loss\"]\n",
    "                losses[\"g_w_loss_acc\"] += loss_dict[\"g_w_loss\"]\n",
    "            \n",
    "\n",
    "            losses[\"c_w_loss_acc\"] /= len(train_data)\n",
    "            losses[\"c_gp_loss_acc\"] /= len(train_data)\n",
    "            losses[\"c_total_loss_acc\"] /= len(train_data)\n",
    "            losses[\"g_w_loss_acc\"] /= len(train_data)\n",
    "            \n",
    "            # print epoch progress\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: c_w_loss: {losses['c_w_loss_acc'] :.4f} \"\n",
    "                f\" c_gp_loss: {losses['c_gp_loss_acc']:.4f} \"\n",
    "                f\" c_total_loss: {losses['c_total_loss_acc']:.4f}\"\n",
    "                f\" g_w_loss: {losses['g_w_loss_acc']:.4f}\"\n",
    "            )\n",
    "            # log loss to tensorboard\n",
    "            self.writer_train.add_scalar(\"c_w_loss\", losses[\"c_w_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"c_gp_loss\", losses[\"c_gp_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"c_total_loss\",losses[\"c_total_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"g_w_loss\", losses[\"g_w_loss_acc\"], global_step=i)\n",
    "            \n",
    "            # run call back functions\n",
    "            if callbacks is not None:\n",
    "                logs = {\"device\":self.device,\n",
    "                        \"generator\":self.generator,\n",
    "                        \"model_state_dict\": self.state_dict(),\n",
    "                        \"loss\": losses\n",
    "                }\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(i, logs=logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImages(Callback):\n",
    "    def __init__(self, num_images, latent_dim, save_dir=\"./gen_examples\"):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.latent_dim = latent_dim\n",
    "        self.save_dir = save_dir\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        device = logs[\"device\"]\n",
    "        generator = logs[\"generator\"]\n",
    "\n",
    "        input_noise = torch.randn((self.num_images, self.latent_dim)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generator.eval()\n",
    "            # scale back to 0 to 255\n",
    "            gen_imgs = generator(input_noise).detach() * 127.5 + 127.5\n",
    "            display(gen_imgs, save_to=self.save_dir+f\"/epoch_{epoch}.png\")\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the WGAN object and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN(Z_DIM, CHANNELS, IMAGE_SIZE, log_dir).to(device)\n",
    "print(wgan.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(wgan, (1, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss_function = wasserstein_generator_loss\n",
    "c_loss_function = wasserstein_critic_loss\n",
    "c_gp_loss = gradient_penalty\n",
    "\n",
    "g_optimizer = optim.Adam(wgan.generator.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])\n",
    "c_optimizer = optim.Adam(wgan.critic.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [GenerateImages(10, Z_DIM, save_dir=sample_dir),\n",
    "             SaveCheckpoint(save_dir=checkpoint_dir, save_every=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_120.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    wgan.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.fit(train_data_loader, epochs=EPOCHS, g_optimizer=g_optimizer, c_optimizer=c_optimizer,\n",
    "          c_w_loss_fn=c_loss_function, c_gp_loss=c_gp_loss, g_w_loss_fn=g_loss_function, device=device, callbacks=callbacks,\n",
    "          gp_lambda=GP_WEIGHT, critic_steps=CRITIC_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new images <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample = torch.randn(size=(10, Z_DIM)).to(device)\n",
    "imgs = wgan.generator(z_sample)\n",
    "display(imgs, cmap=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
