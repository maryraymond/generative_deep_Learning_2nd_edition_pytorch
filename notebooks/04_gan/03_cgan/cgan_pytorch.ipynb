{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤ª Conditional WGAN-GP on CelebA Faces - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/04_gan/03_cgan/cgan.ipynb) for conditional GAN, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The original code is available [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition) and is licensed under the Apache License 2.0._\n",
    "_This implementation is distributed under the Apache License 2.0. See the LICENSE file for details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own Conditional GAN on the CelebA faces dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/04_gan/03_cgan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "from notebooks.utils import display\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 32\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 20 * 100\n",
    "CRITIC_STEPS = 3\n",
    "GP_WEIGHT = 10.0\n",
    "LOAD_MODEL = True\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.9\n",
    "LABEL = \"Blond_Hair\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/celeba-dataset\"\n",
    "labels_file = dataset_dir + \"/list_attr_celeba.csv\"\n",
    "image_dir = dataset_dir + \"/img_align_celeba/img_align_celeba/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a dataset for custom structure images with labels\n",
    "class ImageWLabelsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images_dir, images_list, labels_list, transforms,  label_transforms):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels = labels_list\n",
    "        self.transforms = transforms\n",
    "        self.img_files = images_list\n",
    "        self.labels_transforms = label_transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(self.images_dir + self.img_files[idx]).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        if self.labels_transforms:\n",
    "            label = self.labels_transforms(label)\n",
    "        \n",
    "        return(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels\n",
    "attributes = pd.read_csv(labels_file)\n",
    "print(attributes.columns)\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"Blond_Hair\"\n",
    "IMAGE_HEAD = \"image_id\"\n",
    "attributes[LABEL].head()\n",
    "\n",
    "image_list = attributes[IMAGE_HEAD].to_list()\n",
    "labels_list = attributes[LABEL].to_list()\n",
    "labels_list = [x if x==1 else 0 for x in labels_list]\n",
    "\n",
    "print(len(image_list), len(labels_list))\n",
    "print(image_list[0], labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "def one_hot_encode(label):\n",
    "    return F.one_hot(label, num_classes=CLASSES).float()\n",
    "\n",
    "dataset_wlabels = ImageWLabelsDataset(images_dir=image_dir, images_list=image_list, \n",
    "                              labels_list=labels_list, transforms=transform, label_transforms=one_hot_encode)\n",
    "\n",
    "train_data, _ = torch.utils.data.random_split(dataset_wlabels, [0.1, 0.9])\n",
    "\n",
    "train_data_loader_wlabels = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data_loader_wlabels)\n",
    "sample_images, labels = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_images)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the GAN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, channels, image_size, condition_num_classes):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.condition_num_classes = condition_num_classes\n",
    "        self.input_channels = self.channels + self.condition_num_classes\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # layer group 1\n",
    "        p = self._get_padding_size(input_w=self.image_size, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.input_channels, out_channels=64, \n",
    "                                kernel_size=4, stride=2, padding=p)\n",
    "        \n",
    "        # layer group 2 \n",
    "        p = self._get_padding_size(input_w=self.image_size/2, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "        \n",
    "        # layer group 3\n",
    "        p = self._get_padding_size(input_w=self.image_size/4, stride=2, kernal_size=4)\n",
    "\n",
    "        self.conv_3 = nn.Conv2d(in_channels=128, out_channels=128, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "\n",
    "        # layer group 4\n",
    "        p = self._get_padding_size(input_w=self.image_size/8, stride=2, kernal_size=4)\n",
    "        self.conv_4 = nn.Conv2d(in_channels=128, out_channels=128, \n",
    "                               kernel_size=4, stride=2, padding=p)\n",
    "\n",
    "        self.conv_5 = nn.Conv2d(in_channels=128, out_channels=1, \n",
    "                               kernel_size=4, stride=1, padding=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w /2) - 1) * stride\n",
    "        p = (p - input_w) + kernal_size\n",
    "        p = math.ceil(p/2)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, image, condition):\n",
    "\n",
    "        B = image.shape[0]\n",
    "        num_classes = condition.shape[1]\n",
    "\n",
    "        assert num_classes == self.condition_num_classes\n",
    "\n",
    "        # create the condition in shape B, cond_classes, image_size, image_size\n",
    "        condition = condition.view(B, self.condition_num_classes, 1, 1)\n",
    "        condition = condition.expand(-1, -1, self.image_size, self.image_size)\n",
    "\n",
    "        x = torch.cat([image, condition], dim=1)\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_5(x)\n",
    "\n",
    "        x = x.view((B,1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(CHANNELS, IMAGE_SIZE, CLASSES)\n",
    "\n",
    "print(critic.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(critic, ((1,CHANNELS, IMAGE_SIZE, IMAGE_SIZE), (1, CLASSES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_dim, channels, condition_num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "        self.condition_num_classes = condition_num_classes\n",
    "        self.input_dim = self.num_dim + self.condition_num_classes\n",
    "\n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(in_channels=self.input_dim, out_channels=128, \n",
    "                                               kernel_size=4, stride=1, padding=0, bias=False, output_padding=0)\n",
    "        self.bn_1 = nn.BatchNorm2d(num_features=128, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4, stride=2, kernal_size=4)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        \n",
    "        self.bn_2 = nn.BatchNorm2d(num_features=128, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*2, stride=2, kernal_size=4)\n",
    "        self.conv_trans_3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(num_features=128, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*4, stride=2, kernal_size=4) \n",
    "        self.conv_trans_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "        self.bn_4 = nn.BatchNorm2d(num_features=64, momentum=0.9)\n",
    "\n",
    "        p = self._get_padding_size(input_w=4*8, stride=2, kernal_size=4)\n",
    "        self.conv_trans_5 = nn.ConvTranspose2d(in_channels=64, out_channels=self.channels, \n",
    "                                               kernel_size=4, stride=2, padding=p, output_padding=0, bias=False)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_padding_size(input_w, stride, kernal_size):\n",
    "        p = ((input_w - 1) * stride) / 2\n",
    "        p = p - input_w\n",
    "        p = p + (kernal_size / 2)\n",
    "        p = p + 1/2\n",
    "        return math.floor(p)\n",
    "    \n",
    "    def forward(self, noise, condition):\n",
    "        \n",
    "        B = noise.shape[0]\n",
    "        condition_classes = condition.shape[1]\n",
    "\n",
    "        assert condition_classes == self.condition_num_classes\n",
    "\n",
    "        x = torch.cat([noise, condition], dim=1)\n",
    "\n",
    "        x = x.view((B, self.input_dim, 1, 1))\n",
    "        x = self.conv_trans_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_trans_5(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(Z_DIM, CHANNELS, CLASSES)\n",
    "print(generator.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(generator, ((1, Z_DIM), (1, CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN (nn.Module):\n",
    "    def __init__(self, num_dim, channels, image_size, cond_num_classes, log_dir=\"./log/\"):\n",
    "        super().__init__()\n",
    "        self.num_dim = num_dim\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.cond_num_classes = cond_num_classes\n",
    "        \n",
    "        self.generator = Generator(num_dim, channels, cond_num_classes)\n",
    "        self.critic = Critic(channels, image_size, cond_num_classes)\n",
    "\n",
    "        self.writer_train = SummaryWriter(log_dir + \"/train\")\n",
    "    \n",
    "    # this function will not be used, we just implment it to be able to use\n",
    "    # the torchinfo summary function\n",
    "    def forward(self, noise, condition):\n",
    "        x = self.generator(noise, condition)\n",
    "        x = self.critic(x, condition)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, train_inputs):\n",
    "        # set the dicremenator and generator to training mode\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "\n",
    "        # generate fake images\n",
    "        real_images, labels = train_inputs\n",
    "        B = real_images.shape[0]\n",
    "\n",
    "        # We train the critic more times per step\n",
    "        for i in range(self.critic_steps):\n",
    "            # zero the grads\n",
    "            self.c_optimizer.zero_grad()\n",
    "            input_noise = torch.randn((B, self.num_dim)).to(self.device)\n",
    "\n",
    "            real_images = real_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            fake_images = self.generator(input_noise, labels)\n",
    "\n",
    "            # train discremeantor\n",
    "            fake_pred = self.critic(fake_images, labels)\n",
    "            real_pred = self.critic(real_images, labels)\n",
    "\n",
    "            #calculate dicremenator loss \n",
    "            c_w_loss = self.c_w_loss_fn(fake_pred=fake_pred, real_pred=real_pred)\n",
    "            c_gp_loss = self.c_gp_loss_fn(critic=self.critic, real_images=real_images, fake_images=fake_images, labels=labels)\n",
    "            c_total_loss = c_w_loss + (self.gp_lambda * c_gp_loss)\n",
    "\n",
    "\n",
    "            # calculate gradiants\n",
    "            c_total_loss.backward()\n",
    "            # update waits\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        # zero the grads\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        fake_images = self.generator(input_noise, labels)\n",
    "        # train discremeantor\n",
    "        fake_pred = self.critic(fake_images, labels)\n",
    "\n",
    "        g_w_loss = self.g_w_loss_fn(fake_pred)\n",
    "        #cal gradients\n",
    "        g_w_loss.backward()\n",
    "        #update waits\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        loss_dict = {\"c_w_loss\":c_w_loss.item(), \"c_gp_loss\":c_gp_loss.item(), \n",
    "                     \"c_total_loss\": c_total_loss.item(), \"g_w_loss\":g_w_loss.item()}\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    def fit(self, training_dataloader, epochs, g_optimizer, c_optimizer, \n",
    "            c_w_loss_fn, c_gp_loss, g_w_loss_fn, gp_lambda, device, \n",
    "            critic_steps=3, callbacks=None):\n",
    "        \n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.c_w_loss_fn = c_w_loss_fn\n",
    "        self.g_w_loss_fn = g_w_loss_fn\n",
    "        self.c_gp_loss_fn = c_gp_loss\n",
    "        self.gp_lambda = gp_lambda\n",
    "        self.critic_steps = critic_steps\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "\n",
    "            losses = {\"c_w_loss_acc\": 0,\n",
    "                      \"c_gp_loss_acc\": 0,\n",
    "                      \"c_total_loss_acc\": 0,\n",
    "                      \"g_w_loss_acc\": 0}\n",
    "\n",
    "            # loop over all data in the training set\n",
    "            for train_data in training_dataloader:\n",
    "\n",
    "                # run training_step\n",
    "                loss_dict = self.train_step(train_data)\n",
    "                losses[\"c_w_loss_acc\"] += loss_dict[\"c_w_loss\"]\n",
    "                losses[\"c_gp_loss_acc\"] += loss_dict[\"c_gp_loss\"]\n",
    "                losses[\"c_total_loss_acc\"] += loss_dict[\"c_total_loss\"]\n",
    "                losses[\"g_w_loss_acc\"] += loss_dict[\"g_w_loss\"]\n",
    "            \n",
    "\n",
    "            losses[\"c_w_loss_acc\"] /= len(training_dataloader)\n",
    "            losses[\"c_gp_loss_acc\"] /= len(training_dataloader)\n",
    "            losses[\"c_total_loss_acc\"] /= len(training_dataloader)\n",
    "            losses[\"g_w_loss_acc\"] /= len(training_dataloader)\n",
    "            \n",
    "            # print epoch progress\n",
    "            print(\n",
    "                f\"Epoch {i}/{epochs}: Training: c_w_loss: {losses['c_w_loss_acc'] :.4f} \"\n",
    "                f\" c_gp_loss: {losses['c_gp_loss_acc']:.4f} \"\n",
    "                f\" c_total_loss: {losses['c_total_loss_acc']:.4f}\"\n",
    "                f\" g_w_loss: {losses['g_w_loss_acc']:.4f}\"\n",
    "            )\n",
    "            # log loss to tensorboard\n",
    "            self.writer_train.add_scalar(\"c_w_loss\", losses[\"c_w_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"c_gp_loss\", losses[\"c_gp_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"c_total_loss\",losses[\"c_total_loss_acc\"], global_step=i)\n",
    "            self.writer_train.add_scalar(\"g_w_loss\", losses[\"g_w_loss_acc\"], global_step=i)\n",
    "            \n",
    "            # run call back functions\n",
    "            if callbacks is not None:\n",
    "                logs = {\"device\":self.device,\n",
    "                        \"generator\":self.generator,\n",
    "                        \"model_state_dict\": self.state_dict(),\n",
    "                        \"loss\": losses\n",
    "                }\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(i, logs=logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lose functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_critic_loss(fake_pred, real_pred):\n",
    "    w_loss = torch.mean(fake_pred) - torch.mean(real_pred)\n",
    "    return w_loss\n",
    "\n",
    "def wasserstein_generator_loss(fake_pred):\n",
    "    w_loss = -1 * torch.mean(fake_pred)\n",
    "    return w_loss\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, labels):\n",
    "    B = real_images.shape[0]\n",
    "    alpha = torch.randn((B, 1, 1, 1)).to(real_images.device)\n",
    "    interpolated = (alpha * fake_images) + ((1 - alpha) * real_images)\n",
    "    interpolated.requires_grad_(True)\n",
    "    pred = critic(interpolated, labels)\n",
    "\n",
    "    # calculate the gradient of the output with respect to the input\n",
    "    gradients = autograd.grad(outputs=pred, inputs=interpolated, grad_outputs=torch.ones_like(pred),\n",
    "                  create_graph=True, only_inputs=True)[0]\n",
    "    # flaten the gradients for each image\n",
    "    gradients = gradients.view(B, -1)\n",
    "    # L2 norm\n",
    "    # grad_norm = torch.sqrt(torch.sum(torch.square(gradients)))\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "    gp = torch.mean((grad_norm - 1)**2)\n",
    "\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImages(Callback):\n",
    "    def __init__(self, num_images, latent_dim, save_dir=\"./gen_examples\"):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.latent_dim = latent_dim\n",
    "        self.save_dir = save_dir\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        device = logs[\"device\"]\n",
    "        generator = logs[\"generator\"]\n",
    "\n",
    "        input_noise = torch.randn((self.num_images, self.latent_dim)).to(device)\n",
    "        condition_zero= torch.tensor([[1.0, 0.0]] * self.num_images).to(device)\n",
    "        condition_one = torch.tensor([[0.0, 1.0]] * self.num_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generator.eval()\n",
    "            # scale back to 0 to 255\n",
    "            gen_imgs_labels_zero = generator(input_noise, condition_zero).detach() * 127.5 + 127.5\n",
    "            display(gen_imgs_labels_zero, save_to=self.save_dir+f\"/epoch_{epoch}_lables_0.png\")\n",
    "\n",
    "            gen_imgs_labels_one = generator(input_noise, condition_one).detach() * 127.5 + 127.5\n",
    "            display(gen_imgs_labels_one, save_to=self.save_dir+f\"/epoch_{epoch}_lables_1.png\")\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cgan object and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = CGAN(Z_DIM, CHANNELS, IMAGE_SIZE, CLASSES, log_dir).to(device)\n",
    "print(cgan.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cgan, ((1, Z_DIM), (1, CLASSES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss_function = wasserstein_generator_loss\n",
    "c_loss_function = wasserstein_critic_loss\n",
    "c_gp_loss = gradient_penalty\n",
    "\n",
    "g_optimizer = optim.Adam(cgan.generator.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])\n",
    "c_optimizer = optim.Adam(cgan.critic.parameters(), lr=LEARNING_RATE, betas=[ADAM_BETA_1, ADAM_BETA_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [GenerateImages(10, Z_DIM, save_dir=sample_dir),\n",
    "             SaveCheckpoint(save_dir=checkpoint_dir, save_every=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_600.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    cgan.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.fit(train_data_loader_wlabels, epochs=EPOCHS, g_optimizer=g_optimizer, c_optimizer=c_optimizer,\n",
    "          c_w_loss_fn=c_loss_function, c_gp_loss=c_gp_loss, g_w_loss_fn=g_loss_function, device=device, callbacks=callbacks,\n",
    "          gp_lambda=GP_WEIGHT, critic_steps=CRITIC_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new images <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "z_sample = torch.randn(size=(num_samples, Z_DIM)).to(device)\n",
    "\n",
    "label_0 = torch.tensor([[1., 0.]] * num_samples).to(device)\n",
    "imgs = cgan.generator(z_sample, label_0)\n",
    "display(imgs, cmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = torch.tensor([[0., 1.]] * num_samples).to(device)\n",
    "imgs = cgan.generator(z_sample, label_1)\n",
    "display(imgs, cmap=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
