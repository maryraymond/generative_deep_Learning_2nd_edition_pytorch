{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab632b69",
   "metadata": {},
   "source": [
    "# ðŸš€ GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a318d1e",
   "metadata": {},
   "source": [
    "This notebook is an **unofficial PyTorch implementation** of the excellent [Keras example](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/09_transformer/gpt/gpt.ipynb) for transformers and GPT, originally created by David Foster as part of the companion code for the excellent book [Generative Deep Learning, 2nd Edition](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9995d95",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own GPT model on the Wine Reviews dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e285ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the working directory and the current notebook directory\n",
    "working_dir = os.getcwd()\n",
    "exp_dir = os.path.join(working_dir, \"notebooks/09_transformer/01_gpt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ccc99",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 80\n",
    "# Since PyTorch does not take in the projection size for keys, queries, and values, but rather calculates them as embedding_dim/num_heads,\n",
    "# for our current config of 2 heads, we cannot match Keras in setting the embedding_dim = 256 and the key_dim (internal projection dim)\n",
    "# to 256. It can either be embed_dim = 256 and the key projection will automatically be 128 (less powerful than Keras), or \n",
    "# we can set the embedding to 512 and so the key projection dim will be 256 (more powerful embedding than Keras).\n",
    "EMBEDDING_DIM = 256\n",
    "# EMBEDDING_DIM = 512\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "# N_HEADS = 1\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fba8d6",
   "metadata": {},
   "source": [
    "## 1. Load the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6de1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = working_dir + \"/data\"\n",
    "dataset_dir = data_dir + \"/wine-reviews\"\n",
    "data_file = dataset_dir + \"/winemag-data-130k-v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, \"r\") as json_data:\n",
    "    wine_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [\"wine review : \" +\n",
    "                 f\"{x['country']} : \" +\n",
    "                 f\"{x['province']} : \" +\n",
    "                 f\"{x['variety']} : \" +\n",
    "                 f\"{x['description']}\"\n",
    "                 for x in wine_data if \"country\" in x and x[\"country\"] is not None and\n",
    "                 \"province\" in x and x[\"province\"] is not None and \n",
    "                 \"variety\" in x and x[\"variety\"] is not None and\n",
    "                 \"description\" in x and x[\"description\"] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wine_reviews = len(filtered_data)\n",
    "print(f\"The number of available wine reviews = {n_wine_reviews}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4fa8c",
   "metadata": {},
   "source": [
    "## 2. Tokenize the data <a name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(str):\n",
    "    # add space before and after every punctuation\n",
    "    str = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", str)\n",
    "    # replace multiple spaces with one space\n",
    "    str = re.sub(\" +\", \" \", str)\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b85ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Hello   there!\"\n",
    "test_text = pad_punctuation(test_text)\n",
    "print(test_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = list(map(pad_punctuation, filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5095d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_data[10])\n",
    "print(f\"Text data size = {len(text_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will modify the TextSeqDataset class that we have implemented in chapter 5 for the lstm to internally create the tokenizer and process \n",
    "# string data if required, this will make it more reusable in the future if we want to use the same tokenizer specs\n",
    "class TextSeqAdvancedDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenize=False, \n",
    "                 vocab_size=VOCAB_SIZE, max_seq_len=MAX_LEN,\n",
    "                 verbose=0):\n",
    "        super().__init__()\n",
    "        if tokenize:\n",
    "            # we will use the hugging face Tokenizers package to Tokenize the dataset and create the vocab\n",
    "            # We will use a simple word tokenizer\n",
    "            # the tokenizer itself will handel assigning a numerical value to each word\n",
    "            tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "            # the pre tokenizer will pre process the test and split it into words (based on whitespace)\n",
    "            tokenizer.normalizer = normalizers.Lowercase()\n",
    "            tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "            # to form the vocabilary using the tokenizer we use trainer\n",
    "            trainer = trainers.WordLevelTrainer(special_tokens=[\"<pad>\", \"<unk>\"], vocab_size=vocab_size)\n",
    "            tokenizer.train_from_iterator(text_data, trainer)\n",
    "\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            self.pad_idx = vocab[\"<pad>\"]\n",
    "            self.unk_idx = vocab[\"<unk>\"]\n",
    "\n",
    "            # enable trancation and padding for the dataste so that all entries would have the same length\n",
    "            tokenizer.enable_padding(length=max_seq_len + 1, pad_id=self.pad_idx, pad_token=\"<pad>\")\n",
    "            tokenizer.enable_truncation(max_length=MAX_LEN + 1)\n",
    "\n",
    "            self.vectorized_data_list = [tokenizer.encode(sentence).ids for sentence in data_list]\n",
    "            self.indx_to_word = vocab_idnx_to_word = {vocab[key]: key for key in vocab.keys()}\n",
    "\n",
    "            if verbose == 1:\n",
    "                # print details of the resulting vocabulary\n",
    "                print(\"Vocabulary size:\", tokenizer.get_vocab_size())\n",
    "                print(\"Vocabulary:\", vocab)\n",
    "                print(\"pading index = \", self.pad_idx)\n",
    "                \n",
    "                print(vocab_idnx_to_word)\n",
    "                \n",
    "        else:\n",
    "            self.vectorized_data_list = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.vectorized_data_list))\n",
    "    \n",
    "    def get_pad_idx(self):\n",
    "        return self.pad_idx\n",
    "    \n",
    "    def get_unk_idx(self):\n",
    "        return self.unk_idx\n",
    "    \n",
    "    def get_idx_to_word(self):\n",
    "        return self.indx_to_word\n",
    "    \n",
    "    def get_data_pair(self, idx):\n",
    "        text = self.vectorized_data_list[idx]\n",
    "        x = torch.tensor(text[:-1])\n",
    "        y = torch.tensor(text[1:])\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_data_pair(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will set the value for the token paralization to avoid getting warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "train_dataset = TextSeqAdvancedDataset(data_list=text_data, tokenize=True, \n",
    "                                       vocab_size=VOCAB_SIZE, max_seq_len=MAX_LEN, \n",
    "                                       verbose=1)\n",
    "\n",
    "pad_idx = train_dataset.get_pad_idx()\n",
    "unk_idx = train_dataset.get_unk_idx()\n",
    "vocab_idx_to_word = train_dataset.get_idx_to_word()\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{i}:{vocab_idx_to_word[i]}\")\n",
    "\n",
    "x, y = train_dataset.get_data_pair(0)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de013c",
   "metadata": {},
   "source": [
    "## 3. Create the Training Set <a name=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7e156",
   "metadata": {},
   "source": [
    "## 5. Create the causal attention mask function <a name=\"causal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22416374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(num_keys, num_query, dtype):\n",
    "    #Note 1: In pytorch True means attention disabled and False meanse attention enabled\n",
    "    # this is the opposit convension to Keras \n",
    "\n",
    "    #Note 2: The batch size and batch additional dimention is not required for pytorch since\n",
    "    #  it will broadcat a mask  of shape (seq_len, seq_len) for each bacth and each head\n",
    "    #  else the dim 0 size should be num_heads*batch_size\n",
    "\n",
    "    # following is an implmentation similar to the Keras one\n",
    "    # j = torch.arange(num_keys)\n",
    "    # i = torch.arange(num_query).unsqueeze(1)\n",
    "    # mask = i >= (j - num_query + num_keys)\n",
    "    # mask = ~mask.to(dtype=dtype)\n",
    "\n",
    "    # a more Pytorch like implementaion\n",
    "    mask = torch.triu(torch.ones(num_keys, num_query, dtype=dtype), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention_mask(10, 10, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77308e38",
   "metadata": {},
   "source": [
    "## 6. Create a Transformer Block layer <a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"In Pytorch the size of the projected size (i.e dq, dk and dv) can not be direcly set, instead they are calculated as embedded_dim/num_heads\n",
    "    for the each embedded dimesion of the query, key and value, if the input embedded dimension of the 3 of those is the same we can not set dk with \n",
    "    a different value than dv for this reason the pytorch transformer class will not have a key_dim configuration as in the Keras class, instead\n",
    "    the emded_dim will be the dim of the input embedding sizes and output size and will be used internally by pytorch to calculate the projection size\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, \n",
    "                 embed_dim, ff_dim,\n",
    "                 seq_len, \n",
    "                 dropout_rate=0.1,\n",
    "                 verbose=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.atten = nn.MultiheadAttention(num_heads=self.num_heads, embed_dim=self.embed_dim, batch_first=True)\n",
    "        self.dropout_1 = nn.Dropout(self.dropout_rate)\n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.ff_1 = nn.Linear(in_features=self.embed_dim, out_features=self.ff_dim)\n",
    "        self.ff_2 = nn.Linear(in_features=self.ff_dim, out_features=self.embed_dim)\n",
    "        self.dropout_2 = nn.Dropout(self.dropout_rate)\n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=self.embed_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs, padding_mask=None):\n",
    "        B = inputs.shape[0]\n",
    "        K = inputs.shape[1]\n",
    "        if self.verbose: print(f\"Batch size {B} key and query size {K}\")\n",
    "\n",
    "        device = inputs.device\n",
    "\n",
    "        causal_mask = causal_attention_mask(K, K, torch.bool).to(device)\n",
    "        if self.verbose: print(f\"causal_mask shape : {causal_mask.shape}\")\n",
    "\n",
    "        # we are adding masking of the padding tokens which is not present in the Keras implementation\n",
    "        #  but is supposed to improve the training by not wasting capacity on the padding\n",
    "        atten_output, atten_weights = self.atten(inputs, inputs, inputs, \n",
    "                                                 attn_mask=causal_mask, key_padding_mask=padding_mask)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"atten output shape : {atten_output.shape}\")\n",
    "            print(f\"atten weights shape : {atten_weights.shape}\")\n",
    "\n",
    "        x = self.dropout_1(atten_output)\n",
    "        if self.verbose: print(f\"drop out 1 size : {x.shape}\")\n",
    "\n",
    "        x = self.layer_norm_1(x +  inputs)\n",
    "        if self.verbose: print(f\"layer_norm 1 size : {x.shape}\")\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.ff_1(x)\n",
    "        if self.verbose: print(f\"ff 1 shape : {x.shape}\")\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.ff_2(x)\n",
    "        if self.verbose: print(f\"ff 2 shape {x.shape}\")\n",
    "\n",
    "        x = self.dropout_2(x)\n",
    "\n",
    "        x = self.layer_norm_2(residual + x)\n",
    "        if self.verbose: print(f\"layer_norm 2 size : {x.shape}\")\n",
    "\n",
    "        return x, atten_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_length = 10\n",
    "embed_dim = EMBEDDING_DIM\n",
    "\n",
    "test_sequence = torch.rand(batch_size, seq_length, embed_dim)\n",
    "\n",
    "transformer = TransformerBlock(1, embed_dim, ff_dim=200, seq_len=seq_length, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, atten_weights = transformer(test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765e64c",
   "metadata": {},
   "source": [
    "## 7. Create the Token and Position Embedding <a name=\"embedder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55193fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding (nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, \n",
    "                 embed_dim, pad_idx=0, verbose=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "                                      embedding_dim=self.embed_dim, \n",
    "                                      padding_idx=self.pad_idx)\n",
    "        \n",
    "        self.pos_emb = nn.Embedding(num_embeddings=self.max_seq_len, \n",
    "                                    embedding_dim=self.embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "        #we will add a dimention at 0 to broadcast for the batch dimension\n",
    "        pos_tensor = torch.arange(seq_len).unsqueeze(0).to(device)\n",
    "        pos_embedding = self.pos_emb(pos_tensor)\n",
    "        if self.verbose: print(f\"Embed: pos embed size = {pos_embedding.shape}\")\n",
    "        token_embedding = self.token_emb(x)\n",
    "        if self.verbose: print(f\"Embed: token embed size = {token_embedding.shape}\")\n",
    "        embedding = token_embedding + pos_embedding\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774ff2e",
   "metadata": {},
   "source": [
    "## 8. Build the GPT model <a name=\"transformer_decoder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, \n",
    "                 ff_dim, vocab_size, max_seq_len, \n",
    "                 pad_idx=0, dropout_rate=0.1,\n",
    "                 verbose=0, log_dir = \"./log\",\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "        self.verbose=verbose\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.writter = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        # Creating the GPT layers\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(self.vocab_size, self.max_seq_len, \n",
    "                                                         self.embed_dim, self.pad_idx, verbose=self.verbose)\n",
    "        self.transformer_layer = TransformerBlock(self.num_heads, self.embed_dim, self.ff_dim, \n",
    "                                                  self.max_seq_len, self.dropout_rate, verbose=self.verbose)\n",
    "        # In the forward function we will pass the output from the FF layer through a softmax\n",
    "        # Activation\n",
    "        self.ff_layer = nn.Linear(in_features=self.embed_dim, out_features=self.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        padding_mask = (x == self.pad_idx)\n",
    "\n",
    "        x = self.embedding_layer(x)\n",
    "        if self.verbose: print(f\"GPT: Embedding size = {x.shape}\")\n",
    "\n",
    "        x, atten_weights = self.transformer_layer(x, padding_mask=padding_mask)\n",
    "        if self.verbose:print(f\"GPT: transformer output size = {x.shape}\")\n",
    "        \n",
    "        x = self.ff_layer(x)\n",
    "        if self.verbose:print(f\"GPT: FF output size = {x.shape}\")\n",
    "        \n",
    "        # We will not apply the softmax activaction here since in \n",
    "        # pytorch the CrossEntropyLoss loss function works on raw\n",
    "        # logits and applies the softmax internally\n",
    "        # x = F.softmax(x, dim=-1)\n",
    "        # print(f\"GPT: Softmax output size = {x.shape}\")\n",
    "\n",
    "        return x, atten_weights\n",
    "    \n",
    "    def fit(self, training_dataloader, optimizer, epochs, loss_fn, device, callbacks=None):\n",
    "\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            acc_loss = 0\n",
    "\n",
    "            for training_data in training_dataloader:\n",
    "                # Run one training step\n",
    "                loss =self.train_step(training_data)\n",
    "\n",
    "                acc_loss += loss\n",
    "            \n",
    "            acc_loss /= len(training_dataloader)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: loss = {acc_loss}\")\n",
    "\n",
    "            self.writter.add_scalar(\"train_loss\", acc_loss, global_step=epoch)\n",
    "\n",
    "             # run call back functions\n",
    "            if callbacks is not None:\n",
    "                logs = {\"model\":self,\n",
    "                        \"device\":self.device,\n",
    "                        \"model_state_dict\": self.state_dict(),\n",
    "                        \"loss\": acc_loss\n",
    "                }\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs=logs)\n",
    "    \n",
    "    def train_step(self, training_data):\n",
    "\n",
    "        text_input, text_gt = training_data\n",
    "\n",
    "        # print(\"Train: \")\n",
    "        # train_str = \"\"\n",
    "        # gt_str = \"\"\n",
    "        # for i in range(10):\n",
    "        #     train_str += (vocab_idx_to_word[text_input[0, i].item()] + \" \")\n",
    "        #     gt_str += (vocab_idx_to_word[text_gt[0, i].item()] + \" \")\n",
    "\n",
    "        # print(f\"train data: {train_str}\")\n",
    "        # print(f\"gt data: {gt_str}\")\n",
    "\n",
    "        text_input = text_input.to(self.device)\n",
    "        text_gt = text_gt.to(self.device)\n",
    "\n",
    "        self.train()\n",
    "        # zero the grad to clear any accumulated grads\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pred_text, atten_weights = self(text_input)\n",
    "\n",
    "        vocab_size = pred_text.shape[-1]\n",
    "        pred_text = pred_text.view(-1, vocab_size)\n",
    "        text_gt = text_gt.view(-1)\n",
    "\n",
    "        loss = self.loss_fn(pred_text, text_gt)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_test = GPT(num_heads=N_HEADS, embed_dim=EMBEDDING_DIM, ff_dim=FEED_FORWARD_DIM, \n",
    "               vocab_size=VOCAB_SIZE, max_seq_len=MAX_LEN, verbose=1, log_dir=(exp_dir + \"/log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_length = MAX_LEN\n",
    "embed_dim = EMBEDDING_DIM\n",
    "\n",
    "test_sequence = torch.randint(low=0, high=VOCAB_SIZE, size=(batch_size, seq_length))\n",
    "output, weights = gpt_test(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(gpt_test, (2, 80), device=\"cpu\", dtypes=(torch.int32,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf438a67",
   "metadata": {},
   "source": [
    "## 9. Train the Transformer <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013980a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir =  exp_dir + \"/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "sample_dir =  exp_dir + \"/sample_gen\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir =  exp_dir + \"/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c290e",
   "metadata": {},
   "source": [
    "callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73611777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, save_dir, save_every=10):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.save_every = save_every\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if (epoch % self.save_every) == 0:\n",
    "            checkpoint = {\"epoch\":epoch,\n",
    "                        \"model_state_dict\":logs[\"model_state_dict\"],\n",
    "                        \"loss\":logs[\"loss\"]\n",
    "                        }\n",
    "            checkpoint_file = self.save_dir + f\"/checkpoint_{epoch}.pth\"\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(Callback):\n",
    "    def __init__(self, index_to_word, max_tokens=100, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }  \n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def sample_from(self, probs, temperature):  \n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / torch.sum(probs)\n",
    "        sample_token = torch.multinomial(probs, 1).item()\n",
    "        return sample_token, probs\n",
    "\n",
    "    def generate(self, model, start_prompt, max_tokens, temperature, device):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ] \n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0: \n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.tensor([start_tokens]).to(device)\n",
    "                y, atten_weights = model(x)\n",
    "                y = y.detach().to(\"cpu\") \n",
    "                atten_weights = atten_weights.detach().to(\"cpu\")\n",
    "                # since we output logits with no softmax we will \n",
    "                # apply softmax here to get the probabilities\n",
    "                y_prob = torch.softmax(y, dim=-1)\n",
    "                sample_token, probs = self.sample_from(y_prob[0][-1], temperature)  \n",
    "                info.append({\"prompt\": start_prompt, \n",
    "                             \"word_probs\": probs,\n",
    "                             \"atts\": atten_weights[0, -1, :]})\n",
    "                start_tokens.append(sample_token)  \n",
    "                start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if logs:\n",
    "            model = logs[\"model\"]\n",
    "            device = logs[\"device\"]\n",
    "            self.generate(model, \"wine review\", max_tokens=self.max_tokens, temperature=1.0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1599ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [SaveCheckpoint(save_dir=checkpoint_dir, save_every=2),\n",
    "             TextGenerator(index_to_word=vocab_idx_to_word, max_tokens=MAX_LEN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b17b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ebd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(num_heads=N_HEADS, embed_dim=EMBEDDING_DIM, \n",
    "          ff_dim=FEED_FORWARD_DIM, vocab_size=VOCAB_SIZE, \n",
    "          max_seq_len=MAX_LEN, pad_idx=pad_idx, log_dir=log_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3094a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have checkpoint to load\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_file = checkpoint_dir + \"/checkpoint_10.pth\"\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    gpt.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd46ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=gpt.parameters(), lr=LEARNING_RATE)\n",
    "# unlike the Keras implementation we will ignore the padding value for the loss function so that the model would not learn to \n",
    "# predict padding tokens and to improve the training\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b57b5",
   "metadata": {},
   "source": [
    "Note: the Quality of text generated is not as good as the Keras implementation, and more debugging might be required, this could be due to a few reasons:\n",
    "- The limitation of setting the Key_dim which we can not set to 256 for embedding of 256 and 2 heads\n",
    "- The Keras tokenizer might be more sophisticated than the simple tokenizer used here\n",
    "- The Pytorch training seems to be unstable if a very large learning rate (like 0.001) is used, so we are using a smaller learning rate that needs more epochs to train\n",
    "- We might need to do a few parameters tuning\n",
    "(Improving this is work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.fit(train_dataloader, optimizer=optimizer, \n",
    "        epochs=EPOCHS, loss_fn=loss_fn, \n",
    "        device=device, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(info, vocab, top_k=5):\n",
    "    for i in info:\n",
    "        highlighted_text = []\n",
    "        for word, att_score in zip(\n",
    "            i[\"prompt\"].split(), i[\"atts\"]\n",
    "        ):\n",
    "            highlighted_text.append(\n",
    "                '<span style=\"background-color:rgba(135,206,250,'\n",
    "                + str(att_score.numpy()/max(i[\"atts\"]).item())\n",
    "                + ');\">'\n",
    "                + word\n",
    "                + \"</span>\"\n",
    "            )\n",
    "        highlighted_text = \" \".join(highlighted_text)\n",
    "        display(HTML(highlighted_text))\n",
    "\n",
    "        word_probs = i[\"word_probs\"].numpy()\n",
    "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
    "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "        for p, i in zip(p_sorted, i_sorted):\n",
    "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
    "        print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ea958",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(index_to_word=vocab_idx_to_word, max_tokens=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(\n",
    "    gpt, start_prompt=\"wine review : us\", max_tokens=80, temperature=1.0, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(\n",
    "    gpt, start_prompt=\"wine review : italy\", max_tokens=80, temperature=0.5, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35868116",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(\n",
    "    gpt, start_prompt=\"wine review : germany\", device=device, max_tokens=80, temperature=0.5\n",
    ")\n",
    "print_probs(info, vocab_idx_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
